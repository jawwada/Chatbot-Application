{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87d9915-d966-4fcf-91c0-ca9165d7784d",
   "metadata": {},
   "source": [
    "# Evaluation and Serving of the LSTM with Attention, Improved Performance in Production and Edge Cases\n",
    "This notebook is part 3 of the series, Ultimate-ai-challenge. It is the most fun notebook, because it detects the model failures, and improves the model performance by using chatgpt. Such distillation approaches are appearning in blogs.\n",
    "\n",
    "I continue: after tuning hyperparameters in part 2, I demonstrate the process of evaluating, and serving an LSTM model with attention for intent classification on the ATIS dataset. I evaluate the model using different metrics \n",
    "1. Accuracy\n",
    "2. Precision\n",
    "3. Recall\n",
    "4. Support\n",
    "5. ROC curve, confusion matrix, classification report, etc..\n",
    "\n",
    "One of the critical outcomes of this work how do I measure performance during production and then take improvement measures.\n",
    "## Improving model performace in production and handling edge cases \n",
    "also add some test/edge cases to show that how the model performance can be improved in production or how the out-of-sample detection can be made better. There can be many strategies to check the model performance in production. However I only work on two\n",
    "### 1. Confidence Scores\n",
    "### 2. Out-of-Sample Detection\n",
    "\n",
    "\n",
    "Lets start!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf90476f-d405-4944-8d6e-7c0fa3457cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/ahmedjawad/Documents/JobSearch/UltimateAIChallenge/ultimate_aiv2\n"
     ]
    }
   ],
   "source": [
    "### Important Note; carefully set the project Root, so relative class imports work.\n",
    "import os\n",
    "marker = '.git'  # Replace with your unique marker file or directory\n",
    "while not os.path.exists(marker):\n",
    "    os.chdir('..')\n",
    "# Verify the current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25fef6cb-3bba-4d57-aa47-63c02da864e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:07.771433Z",
     "start_time": "2023-11-23T11:55:06.807481Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'machine_learning.learner'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmachine_learning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mIntentClassifierLSTMWithAttention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntentClassifierLSTMWithAttention\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'machine_learning.learner'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from machine_learning.learners.IntentTokenizer import IntentTokenizer\n",
    "from machine_learning.learners.model_utils import train, evaluate, predict, get_or_create_experiment\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "import warnings\n",
    "import mlflow\n",
    "from machine_learning.learners.model_utils import get_or_create_experiment\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from machine_learning.learner.IntentClassifierLSTMWithAttention import IntentClassifierLSTMWithAttention\n",
    "import torch\n",
    "import optuna\n",
    "import logging\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "# Reload Optuna Study to get the best parameters\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cae25d-0bbe-415a-b5b3-c51c28920692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:07.790202Z",
     "start_time": "2023-11-23T11:55:07.788622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Determine the computation device based on availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bac532",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Load the tokenizers and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d2802-1167-4174-ab57-99e076566c7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:07.807724Z",
     "start_time": "2023-11-23T11:55:07.792313Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the model name for loading\n",
    "from intent_classifier import IntentClassifier\n",
    "# Load the trained model and tokenizer from saved files\n",
    "batch_size=32\n",
    "model_name = \"best_ICELSTMAmodel\"\n",
    "model = torch.load(f\"data/models/{model_name}.pth\").to(device)\n",
    "tokenizer = IntentTokenizer.load_state(IntentTokenizer,f\"data/models/{model_name}_tokenizer.pickle\", f\"data/models/{model_name}_le.pickle\")\n",
    "test_df = pd.read_csv('data/input/atis/test.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "test_data = tokenizer.process_data(test_df, device=device)\n",
    "print(\"Number of test samples:\", test_data.tensors[0].size())\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9942fc7e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Model Metrics: Evaluate the model on the test dataset\n",
    "The following methods are used for evaluation:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "- Confusion Matrix\n",
    "- Precision-Recall Curve\n",
    "- ROC Curve\n",
    "- Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab6bf1-71f9-4039-8410-47806d23a1ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:08.128559Z",
     "start_time": "2023-11-23T11:55:07.809355Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract true labels from the test data\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# Predict labels using the trained model\n",
    "y_pred = predict(model, test_df, tokenizer, device)\n",
    "\n",
    "# Compute accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Display the computed metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246b8ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Visualize the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06374a6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:08.618611Z",
     "start_time": "2023-11-23T11:55:08.131799Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b22a3-3bbe-4d94-995d-32ed6a8ca8f5",
   "metadata": {},
   "source": [
    "One can make interesting observations about class distributions and prediction failures from confusion matrix. Let's discuss laters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67345e5e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Visualize the precision-recall curve and ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19c8e2-576f-4d40-9a9c-888cec9090a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:08.860635Z",
     "start_time": "2023-11-23T11:55:08.622760Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Binarize the labels for multi-class plots\n",
    "y_test_binarized = label_binarize(y_test, classes=np.unique(y_test))\n",
    "y_pred_binarized = label_binarize(y_pred, classes=np.unique(y_test))\n",
    "\n",
    "n_classes = y_test_binarized.shape[1]\n",
    "\n",
    "# Plot Precision-Recall curve for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(n_classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_binarized[:, i], y_pred_binarized[:, i])\n",
    "    plt.plot(recall, precision, lw=2, label=f'class {tokenizer.le.classes_[i]}')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Precision vs. Recall curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c45b73",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Visualize ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb022a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:09.023715Z",
     "start_time": "2023-11-23T11:55:08.867886Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(n_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_binarized[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'Class {tokenizer.le.classes_[i]} (area = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192fdf7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc661407",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Serve the model for prediction , Evaluate Production Performance**\n",
    "\n",
    "1. **Confidence Score Analysis** :\n",
    "Confidence Distribution: Analyze the distribution of confidence scores for predictions. A well-performing model should generally have higher confidence scores.\n",
    "Threshold Analysis: Determine how varying confidence thresholds affect the number of predicted intents. A stable model will show a consistent pattern in the distribution of predictions above certain confidence levels.\n",
    "\n",
    "2. **Error Analysis**\n",
    "Manual Review: Randomly sample a set of predictions and manually review them. This can provide qualitative insights into types of errors (e.g., systematic errors, rare cases).\n",
    "Consistency Check: For applications with user feedback loops (like chatbots), monitor user actions following the model's response. Frequent user corrections or negative feedback can indicate issues.\n",
    "\n",
    "3. **Out-of-Distribution (OOD) Detection**\n",
    "OOD Samples: Use a separate dataset that the model hasn't seen, preferably containing out-of-distribution samples. Measure how the model's confidence scores differ on these samples.\n",
    "OOD Metrics: Evaluate using metrics like Area Under the Receiver Operating Characteristic curve (AUROC) for OOD detection efficiency.\n",
    "\n",
    "4. **Comparative Analysis**\n",
    "A/B Testing: If possible, implement A/B testing where different models serve different user segments. Monitor indirect metrics like user engagement, session length, or conversion rates.\n",
    "Sequential Testing: Periodically switch between models in production and monitor overall system performance metrics.\n",
    "\n",
    "5. **Model Drift Monitoring**\n",
    "Data Drift: Monitor for shifts in input data distribution over time. Significant drifts can suggest that the model might be becoming less effective.\n",
    "rediction Drift: Monitor changes in the distribution of the model's predictions. A sudden shift could indicate that the model's performance is degrading.\n",
    "\n",
    "6. **Feedback Loop**\n",
    "User Feedback: Implement a mechanism for users to provide feedback on predictions. This can be direct (like thumbs up/down) or inferred (like user ignoring the model's suggestion).\n",
    "\n",
    "7. **Additional Metrics**\n",
    "Latency: Measure the response time. A slower model might impact user experience negatively.\n",
    "Resource Utilization: Monitor CPU, memory, and other resource usages. An efficient model should balance accuracy with resource consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e15a87-6588-4365-85e8-f1d5be1029c7",
   "metadata": {},
   "source": [
    "# Production time serving and edge cases\n",
    "### Implementing confidence scores\n",
    "### 1. Extracting Confidence Scores:\n",
    "When making predictions with your model, you'll usually obtain not only the predicted labels but also the probabilities associated with each class. These probabilities can be interpreted as confidence scores.\n",
    "For a given input text, the model outputs a confidence score for each possible intent. The intent with the highest score is typically chosen as the prediction.\n",
    "### 2. Analyzing Confidence Scores:\n",
    "Histogram of Confidence Scores: Plot a histogram of the confidence scores of all predictions. This will give you an idea of how certain the model is about its predictions. A well-performing model will have a distribution skewed towards higher confidence scores.\n",
    "Average Confidence Score: Calculate the average confidence score across all predictions. A higher average indicates higher overall confidence of the model in its predictions.\n",
    "### 3. Confidence Threshold Analysis:\n",
    "Determine how different confidence thresholds affect the number of predicted intents and their perceived accuracy. For example, you might analyze how many predictions have confidence above 0.9, 0.8, etc.\n",
    "Observing the drop-off in the number of predictions above certain confidence levels can provide insights into the modelâ€™s reliability.\n",
    "### 4. Segment Analysis:\n",
    "Break down the confidence scores by different segments such as time of day, user demographics, or type of query. This can help identify if the model performs consistently across different segments or if there are specific areas where confidence is notably lower.\n",
    "### 6. Interpretation:\n",
    "High Confidence, Poor Accuracy: If the model shows high confidence but poor accuracy, this might indicate overfitting or issues with the training data.\n",
    "Low Confidence: Consistently low confidence scores might suggest the model is underfitting or not complex enough to capture the nuances of the data.\n",
    "### 7. Use in Production:\n",
    "Implement logging of confidence scores in your production system.\n",
    "Regularly analyze these logs to monitor model performance.\n",
    "Use the insights from this analysis for model retraining or adjustment decisions.\n",
    "Remember, this analysis is part of a continuous monitoring strategy and should be adapted and expanded based on specific use cases and available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b04c98-73c9-454c-bb89-c698702dafc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:09.500410Z",
     "start_time": "2023-11-23T11:55:09.023522Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size=32\n",
    "\n",
    "\n",
    "\n",
    "confidence_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        max_probs, _ = torch.max(probabilities, dim=1)\n",
    "        confidence_scores.extend(max_probs.cpu().numpy())\n",
    "\n",
    "# Plotting histogram of confidence scores\n",
    "plt.hist(confidence_scores, bins=50, alpha=0.7, color='blue')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Confidence Scores')\n",
    "plt.show()\n",
    "\n",
    "# Calculating and printing average confidence score\n",
    "average_confidence = np.mean(confidence_scores)\n",
    "print(f'Average Confidence Score: {average_confidence:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c2427c-a85b-4ed1-8636-8a5804f6026f",
   "metadata": {},
   "source": [
    "# OOS case: \"I want to book a hotel room near Miami Beach\", \n",
    "## How will the model respond?\n",
    "\n",
    "The first guess is that the model will be wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940ca82-5300-4676-adab-0ca154ca3503",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:09.556269Z",
     "start_time": "2023-11-23T11:55:09.488526Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Your query string\n",
    "query_str = \"I want to book a hotel room near Miami Beach\"\n",
    "# Creating a DataFrame with the query string\n",
    "query = pd.DataFrame([query_str], columns=['text'])\n",
    "y_pred= predict(model, query, tokenizer, device)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a19937-0548-46bd-8057-1c327383f224",
   "metadata": {},
   "source": [
    "# BAD NEWS!! Booking a hotel room has no relation to flights.\n",
    "Out-of-Sample Detection is poor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11424e4b-b33e-40f2-8a03-abe5a63aad53",
   "metadata": {},
   "source": [
    "# Out-of-Distribution (OOD) Detection \n",
    "\n",
    "is crucial for neural network models, especially in a production environment, to identify when a model is making predictions on data that is significantly different from the data it was trained on. Here's how you can approach OOD detection:\n",
    "\n",
    "#### I created two Out-of-Sample files ood.tsv and ood_test.csv by asking chatgpt to include some edge cases. Their purpose will become clear later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff471bf-7058-4b3c-8c69-919ced524ecb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:11.781456Z",
     "start_time": "2023-11-23T11:55:09.535864Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size=32\n",
    "# Load the trained model and tokenizer from saved files\n",
    "model_name = \"best_ICELSTMAmodel\"\n",
    "model = torch.load(f\"data/models/{model_name}.pth\").to(device)\n",
    "def get_confidence_scores(model, data_loader):\n",
    "    model.eval()\n",
    "    confidence_scores = []\n",
    "    with torch.no_grad():\n",
    "        for inputs,_ in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            max_probs, _ = torch.max(probabilities, dim=1)\n",
    "            confidence_scores.extend(max_probs.cpu().numpy())\n",
    "    return confidence_scores\n",
    "\n",
    "train_df = pd.read_csv('data/input/atis/train.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "train_data = tokenizer.process_data(train_df, device=device)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "ood_df=pd.read_csv('data/atis/ood.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "ood_data = tokenizer.process_data(ood_df, device=device)\n",
    "ood_loader= DataLoader(ood_data, shuffle=True, batch_size=batch_size)\n",
    "print(\"Number of ood samples:\", ood_data.tensors[0].size())\n",
    "\n",
    "\n",
    "ood_test_df=pd.read_csv('data/atis/ood_test.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "ood_test_data = tokenizer.process_data(ood_test_df, device=device)\n",
    "ood_test_loader= DataLoader(ood_test_data, shuffle=True, batch_size=batch_size)\n",
    "print(\"Number of ood samples:\", ood_test_data.tensors[0].size())\n",
    "\n",
    "\n",
    "# Assuming train_loader and ood_loader are DataLoader objects for your in-distribution and OOD data\n",
    "train_confidence_scores = get_confidence_scores(model, train_loader)\n",
    "ood_confidence_scores = get_confidence_scores(model, ood_loader)\n",
    "ood_test_confidence_scores = get_confidence_scores(model, ood_test_loader)\n",
    "\n",
    "\n",
    "# Creating subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plotting histograms on subplots\n",
    "axs[0].hist(train_confidence_scores, bins=50, alpha=0.5, label='In-Distribution')\n",
    "axs[0].set_title('In-Distribution Confidence Scores')\n",
    "axs[0].set_xlabel('Confidence Score')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].hist(ood_confidence_scores, bins=50, alpha=0.5, label='OOD')\n",
    "axs[1].set_title('OOD Confidence Scores')\n",
    "axs[1].set_xlabel('Confidence Score')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].hist(ood_test_confidence_scores, bins=50, alpha=0.5, label='OOD')\n",
    "axs[2].set_title('OOD TEST Confidence Scores')\n",
    "axs[2].set_xlabel('Confidence Score')\n",
    "axs[2].set_ylabel('Frequency')\n",
    "axs[2].legend()\n",
    "# Displaying the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fbf76-19fd-44a8-ab86-a0fa638e81f9",
   "metadata": {},
   "source": [
    "### CHECK THE ACTUAL PREDICTIONS ON OOD AND OOD_TEST,\n",
    "As you can see above, the confidence for OOS samples is poor. Below, all non related samples are labelled and not assigned unknown buckets. This is poor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89efc07-5a05-447d-b21e-0933dcf18eb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:11.981568Z",
     "start_time": "2023-11-23T11:55:11.782760Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"How the model performs on OOD data\")\n",
    "y_pred=predict(model, ood_df, tokenizer, device)\n",
    "print(y_pred)\n",
    "\n",
    "print(\"How the model performs on OOD data Test\")\n",
    "y_pred=predict(model, ood_test_df, tokenizer, device)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011399fa-9aa0-4c21-bf06-04cd57c8367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2c20e-e0d1-4638-99a5-e26814f1361c",
   "metadata": {},
   "source": [
    "### Improving the model by adding an on ood_sample in the training data\n",
    "The output above shows very worrying picture, the whole out of sample data is labelled according to known class. Therefore to make the out-of-sample detection better, I generate some fake training data for out of sample using chatgpt and add it to the training data to get ood detection better. This is akin to fine tuning with additional data in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b33409-29f6-4fe9-a9f1-bddf55f07d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_class_name = \"IntentClassifierLSTMWithAttention\"\n",
    "model_name = \"best_ICELSTMAmodel\"\n",
    "model = torch.load(f\"data/models/{model_name}.pth\").to(device)\n",
    "experiment_id = get_or_create_experiment(model_class_name)\n",
    "mlflow.set_experiment(experiment_id=experiment_id)\n",
    "storage = optuna.storages.RDBStorage(url=f\"sqlite:///data/db/{model_class_name}.db\")\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout));\n",
    "study = optuna.create_study(study_name=model_class_name, storage=storage,load_if_exists=True,direction=\"maximize\");\n",
    "best_trial = study.best_trial\n",
    "print(f'Best trial: score {best_trial.value:.4f},\\nparams {best_trial.params}')\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('data/input/atis/train.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "ood_df=pd.read_csv('data/atis/ood.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "ood_data = tokenizer.process_data(ood_df, device=device)\n",
    "ood_loader= DataLoader(ood_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "## *****Adding OOD Data to Training Data ****##\n",
    "train_df=pd.concat([train_df,ood_df])\n",
    "train_data = tokenizer.process_data(train_df, device=device)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "with mlflow.start_run():\n",
    "    # Log the best parameters\n",
    "    lr=best_trial.params['lr']\n",
    "    weight_decay=best_trial.params['lr']\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "    train_loss = train(model, optimizer, nn.CrossEntropyLoss(), train_loader, 20)\n",
    "    test_accuracy = evaluate(model, nn.CrossEntropyLoss(), test_loader, data_type=\"Test\")\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"train_loss\", train_loss)\n",
    "    mlflow.pytorch.log_model(model, f\"best_model_ood\")\n",
    "# Saving the model to avoid ambiguity later\n",
    "\n",
    "print(f\"model_class_name={model_class_name}\")\n",
    "model_name = \"best_ICELSTMAmodel_ood\"\n",
    "torch.save(model, f\"data/models/{model_name}.pth\")\n",
    "print(\"model: \"+model_name+\" is saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224880a-18af-48c0-ac88-68c6d3a749f2",
   "metadata": {},
   "source": [
    "***The Best Model in terms of hyperparameter selection is fine tuned on the out-of-sample data created by chatgpt, a distillation process***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898171e-81f7-4b77-8bf9-a432cc62d601",
   "metadata": {},
   "source": [
    "### OOS test file is still unseen from the model. I check confidence scores on performance on it\n",
    "\n",
    "As you can see the confidence score and performance has improved a lot in the following analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81788899-8ce5-4d9e-b79f-f1e6019d0267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:43.031140Z",
     "start_time": "2023-11-23T11:55:40.411337Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size=5000\n",
    "model_name = \"best_ICELSTMAmodel_ood\"\n",
    "model=torch.load(f\"data/models/{model_name}.pth\")\n",
    "\n",
    "def get_confidence_scores(model, data_loader):\n",
    "    model.eval()\n",
    "    confidence_scores = []\n",
    "    with torch.no_grad():\n",
    "        for inputs,_ in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            max_probs, _ = torch.max(probabilities, dim=1)\n",
    "            confidence_scores.extend(max_probs.cpu().numpy())\n",
    "    return confidence_scores\n",
    "\n",
    "train_df = pd.read_csv('data/input/atis/train.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "train_data = tokenizer.process_data(train_df, device=device)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "ood_df=pd.read_csv('data/atis/ood.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "ood_data = tokenizer.process_data(ood_df, device=device)\n",
    "ood_loader= DataLoader(ood_data, shuffle=True, batch_size=batch_size)\n",
    "print(\"Number of ood samples:\", ood_data.tensors[0].size())\n",
    "\n",
    "\n",
    "ood_test_df=pd.read_csv('data/atis/ood_test.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "ood_test_data = tokenizer.process_data(ood_test_df, device=device)\n",
    "ood_test_loader= DataLoader(ood_test_data, shuffle=True, batch_size=batch_size)\n",
    "print(\"Number of ood samples:\", ood_test_data.tensors[0].size())\n",
    "\n",
    "\n",
    "# Assuming train_loader and ood_loader are DataLoader objects for your in-distribution and OOD data\n",
    "train_confidence_scores = get_confidence_scores(model, train_loader)\n",
    "ood_confidence_scores = get_confidence_scores(model, ood_loader)\n",
    "ood_test_confidence_scores = get_confidence_scores(model, ood_test_loader)\n",
    "\n",
    "\n",
    "# Creating subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plotting histograms on subplots\n",
    "axs[0].hist(train_confidence_scores, bins=50, alpha=0.5, label='In-Distribution')\n",
    "axs[0].set_title('In-Distribution Confidence Scores')\n",
    "axs[0].set_xlabel('Confidence Score')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].hist(ood_confidence_scores, bins=50, alpha=0.5, label='OOD')\n",
    "axs[1].set_title('OOD Confidence Scores')\n",
    "axs[1].set_xlabel('Confidence Score')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].hist(ood_test_confidence_scores, bins=50, alpha=0.5, label='OOD')\n",
    "axs[2].set_title('OOD TEST Confidence Scores')\n",
    "axs[2].set_xlabel('Confidence Score')\n",
    "axs[2].set_ylabel('Frequency')\n",
    "axs[2].legend()\n",
    "# Displaying the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d0b393-9b32-49c5-9700-cfc065e22a9c",
   "metadata": {},
   "source": [
    "### Confidence and Predictions Improved for OODs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a002d14-bdbb-4f31-a863-e84751d5274b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:43.130650Z",
     "start_time": "2023-11-23T11:55:43.031687Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"How the model performs on OOD data\")\n",
    "y_pred=predict(model, ood_df, tokenizer, device)\n",
    "print(y_pred)\n",
    "\n",
    "print(\"How the model performs on OOD data Test\")\n",
    "y_pred=predict(model, ood_test_df, tokenizer, device)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe943e-6183-4759-abc6-6b2f86e2b103",
   "metadata": {},
   "source": [
    "***The Performance is much improved on the unseen data now. I did LLM fine tuning here :)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c556f1-6673-4574-8fec-2735d7f3b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as our final model for serving\n",
    "class_name=model.__class__.__name__\n",
    "print(class_name)\n",
    "print(f\"class_name={class_name}\")\n",
    "model.save_config_file(f\"config/{class_name}.json\")\n",
    "torch.save(model.state_dict(),f\"data/models/{class_name}_state_dict.pth\")\n",
    "tokenizer.save_state(f\"data/models/{class_name}_tokenizer.pickle\", f\"data/models/{class_name}_le.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea71cf-5a64-4226-8e1d-cc8f0a9165a7",
   "metadata": {},
   "source": [
    "## Check close call/Edge messages and model responses\n",
    "Check \"I want to book a hotel room near Miami Beach\" by both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a1a04-2009-4af8-97f4-5d8238b0195c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:43.130885Z",
     "start_time": "2023-11-23T11:55:43.072761Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"best_ICELSTMAmodel\"\n",
    "\n",
    "# Load the trained model and tokenizer from saved files\n",
    "model_without_ood = torch.load(f\"data/models/{model_name}.pth\").to(device)\n",
    "model_with_ood = torch.load(f\"data/models/{model_name}_ood.pth\").to(device)\n",
    "\n",
    "# Your query string\n",
    "query_str = \"I want to book a hotel room near miami beach\"\n",
    "\n",
    "# Creating a DataFrame with the query string\n",
    "query = pd.DataFrame([query_str], columns=['text'])\n",
    "print(\"Model trained without OOD, Response=\" + predict(model_without_ood, query, tokenizer, device))\n",
    "print(\"Model trained with OOD, Response=\" + predict(model_with_ood, query, tokenizer, device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36069f5-ea8d-48dd-9b1c-2c3585bde0e7",
   "metadata": {},
   "source": [
    "## Check \"what a bizzare situation, I had not expected it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8667196-111a-4da9-8bd7-4e4897b153fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:43.561708Z",
     "start_time": "2023-11-23T11:55:43.515254Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"best_ICELSTMAmodel\"\n",
    "\n",
    "# Load the trained model and tokenizer from saved files\n",
    "model_without_ood = torch.load(f\"data/models/{model_name}.pth\").to(device)\n",
    "# Your query string\n",
    "query_str = \"what a bizzare situation this is bot\"\n",
    "\n",
    "# Creating a DataFrame with the query string\n",
    "query = pd.DataFrame([query_str], columns=['text'])\n",
    "print(\"Model trained without OOD, Response=\" + predict(model_without_ood, query, tokenizer, device))\n",
    "print(\"Model trained with OOD, Response=\" + predict(model_with_ood, query, tokenizer, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42766f-4128-45a7-a76a-5a10dda183f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T11:55:43.513897Z",
     "start_time": "2023-11-23T11:55:43.094197Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"best_ICELSTMAmodel\"\n",
    "\n",
    "# Load the trained model and tokenizer from saved files\n",
    "model_without_ood = torch.load(f\"data/models/{model_name}.pth\").to(device)\n",
    "# Your query string\n",
    "query_str = \"Book me a flight from miami to chicago\"\n",
    "\n",
    "# Creating a DataFrame with the query string\n",
    "query = pd.DataFrame([query_str], columns=['text'])\n",
    "print(\"Model trained without OOD, Response=\" + predict(model_without_ood, query, tokenizer, device))\n",
    "print(\"Model trained with OOD, Response=\" + predict(model_with_ood, query, tokenizer, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ddef9-21c0-4387-98d1-3ef56c357d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"best_ICELSTMAmodel\"\n",
    "\n",
    "# Load the trained model and tokenizer from saved files\n",
    "model_without_ood = torch.load(f\"data/models/{model_name}.pth\").to(device)\n",
    "# Your query string\n",
    "query_str = \"life would be good if you had a seat for me in the plane\"\n",
    "\n",
    "# Creating a DataFrame with the query string\n",
    "query = pd.DataFrame([query_str], columns=['text'])\n",
    "print(\"Model trained without OOD, Response=\" + predict(model_without_ood, query, tokenizer, device))\n",
    "print(\"Model trained with OOD, Response=\" + predict(model_with_ood, query, tokenizer, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886577c0-e3d1-4775-afbc-9ff0b41720f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66286f-020c-4bdf-9332-0d6f2ec1f009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
