{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534ec710-e8e4-4a43-887e-593d4ebe20d1",
   "metadata": {},
   "source": [
    "# Intent Classification Model on Atis Data using \n",
    "I plan to build a series of models. \n",
    "1. barebone model for Bench Marking\n",
    "2. Attention based model for trying deep architecture\n",
    "3. Using advance learning libraries to check how the performance of the two compare\n",
    "\n",
    "\n",
    "First importies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe401331",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:34:33.302413Z",
     "start_time": "2023-11-21T02:34:31.919382Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648bd5dd-1f83-4e3e-bee6-ef3e5e80fe62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:34:33.318041Z",
     "start_time": "2023-11-21T02:34:33.314872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea481a-c956-48af-821b-91787297d0f5",
   "metadata": {},
   "source": [
    "# Barebone NLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3ec35-6c92-4847-9e3a-753e9c798557",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "The process of getting a language agnostic representation of text is called tokenization. In this section, we will tokenize the text and build a vocabulary. We will also encode the labels as numerical values. This will help build an intent classifier model that an be used for chatbots in any domain and language\n",
    "### Tokenize and Build Vocabulary\n",
    "\n",
    "### Get the Indexed and Padded Sequences as Tensors\n",
    "\n",
    "### Encode Labels\n",
    "Also handle unknown labels, print a list of classes in the data\n",
    "\n",
    "### Text to Index Lists\n",
    "The text_to_indices function takes a string of text and the word_to_index dictionary.\n",
    "It tokenizes the text into words (using the tokenize function we assumed earlier).\n",
    "For each word in the tokenized text, it finds the corresponding index from the word_to_index dictionary. If the word is not found, it uses the index for \"<UNK>\".\n",
    "The function returns a list of indices representing the text.\n",
    "Finally, we use list comprehensions to apply this function to all entries in the training and testing datasets.\n",
    "After executing this code, train_indices and test_indices will be lists of lists, where each inner list is a sequence of word indices corresponding to a sentence in your training and testing datasets, respectively. These are now ready to be padded and then used for training your PyTorch model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffda9ad-ae5e-4ef9-93bb-400f7ae1e52c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:34:34.505804Z",
     "start_time": "2023-11-21T02:34:33.320106Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 890\n"
     ]
    }
   ],
   "source": [
    "from utils import tokenize, build_vocabulary, text_to_indices, encode_labels, convert_and_pad_sequences\n",
    "\n",
    "# Load the training data\n",
    "train = pd.read_csv(\"data/atis/train.tsv\",sep='\\t', header=None)\n",
    "train.columns = [\"text\", \"label\"]\n",
    "test= pd.read_csv(\"data/atis/test.tsv\",sep='\\t', header=None)\n",
    "test.columns = [\"text\", \"label\"]\n",
    "\n",
    "#build vocabulary\n",
    "vocab_size=1000\n",
    "word_to_index = build_vocabulary(train[\"text\"], vocab_size)\n",
    "print(f\"Vocabulary Size: {len(word_to_index)}\")\n",
    "\n",
    "#get the indexed and padded sequences as tensors\n",
    "train_indices = [text_to_indices(text, word_to_index) for text in train[\"text\"]]\n",
    "test_indices = [text_to_indices(text, word_to_index) for text in test[\"text\"]]\n",
    "train_padded=convert_and_pad_sequences(train_indices,device)\n",
    "test_padded=convert_and_pad_sequences(test_indices,device)\n",
    "# Now, train_padded and test_padded are the padded sequence tensors\n",
    "print(\"Padded Training Sequences:\", train_padded.size())\n",
    "print(\"Padded Testing Sequences:\", test_padded.size())\n",
    "\n",
    "# Convert labels to numerical values\n",
    "le = encode_labels(train,test)\n",
    "train_labels = le.transform(train[\"label\"])\n",
    "test_labels = le.transform(test[\"label\"])\n",
    "print(\"Label Encoding:\", dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27009727-45eb-4986-b003-870298739477",
   "metadata": {},
   "source": [
    "Label Encoding: {'abbreviation': 0, 'aircraft': 1, 'aircraft+flight+flight_no': 2, 'airfare': 3, 'airfare+flight_time': 4, 'airline': 5, 'airline+flight_no': 6, 'airport': 7, 'capacity': 8, 'cheapest': 9, 'city': 10, 'distance': 11, 'flight': 12, 'flight+airfare': 13, 'flight_no': 14, 'flight_time': 15, 'ground_fare': 16, 'ground_service': 17, 'ground_service+ground_fare': 18, 'meal': 19, 'quantity': 20, 'restriction': 21, \"<unknown>\": 22}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb4f71",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb87f36-d2ab-460c-bffe-00ba2b4fbd86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:34:34.510159Z",
     "start_time": "2023-11-21T02:34:34.508076Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Create TensorDatasets\n",
    "# TensorDataset combines a dataset and a label, and provides an iterable over the given dataset. The arguments should be tensors of the same size in the 0th dimension. Any other dimension will be considered as the sample dimension and will be iterated along. This dataset is especially useful to wrap tensors that represent input and target or that are already in batches, e.g. for mini-batch SGD.\n",
    "train_data = TensorDataset(train_padded.to(device), torch.tensor(train_labels).to(device))\n",
    "test_data = TensorDataset(test_padded.to(device), torch.tensor(test_labels).to(device))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "print(\"Number of training batches:\", len(train_loader))\n",
    "print(\"Number of test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f8cfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:34:34.903353Z",
     "start_time": "2023-11-21T02:34:34.892709Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the class distribution ins test and train\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "train_df.label.value_counts().plot(kind='bar', ax=ax1)\n",
    "ax1.set_title(\"Train\")\n",
    "test_df.label.value_counts().plot(kind='bar', ax=ax2)\n",
    "ax2.set_title(\"Test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05c849b-3dc0-4dcd-862a-3ad69682047e",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Yay, our data is now vectorized. We can start playing with it. The first step is to build an embedding model. In this code:\n",
    "\n",
    "vocab_size is the number of unique words in your vocabulary.\n",
    "embedding_dim is the number of dimensions for each word embedding.\n",
    "SimpleNLPModel is a basic PyTorch model class with an embedding layer.\n",
    "The forward method defines how data passes through the model. In this simple example, it only passes through the embedding layer.\n",
    "Finally, an example input is passed through the model to obtain embeddings. The input should be a tensor of token indices, like the output of your padding step.\n",
    "This setup will initialize the embeddings randomly, and they will be updated during training. If you have pre-trained embeddings that you want to use, you can initialize the nn.Embedding layer with these pre-trained weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb73f06-014a-4a44-9355-cf0ff793b0a0",
   "metadata": {},
   "source": [
    "Great!. The Embeddings are working. , if we want to use advance embeddings, we can also use pretrained embeddings, including contextualized embeddings from the task. Lets refine our Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958c25f-ca17-4f7c-91a7-4099c5d841a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:34:41.628433Z",
     "start_time": "2023-11-21T02:34:38.899831Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class IntentClassifierLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(IntentClassifierLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # Batch normalization layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Dropout layer\n",
    "        dropped = self.dropout1(embedded)\n",
    "\n",
    "        # LSTM layer\n",
    "        lstm_out, (hidden, _) = self.lstm(dropped)\n",
    "        # Take the output of the last time step\n",
    "        hidden = hidden[-1]\n",
    "        # Batch normalization\n",
    "        normalized = self.batch_norm(hidden)\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(normalized)\n",
    "        return out\n",
    "\n",
    "# Instantiate the model with dropout and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5e703-a7f5-49b4-8cd3-8de90e57c983",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:34:41.643824Z",
     "start_time": "2023-11-21T02:34:41.633417Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, feature_size):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        # Linear transformations for Q, K, V from the same source\n",
    "        self.key = nn.Linear(feature_size, feature_size)\n",
    "        self.query = nn.Linear(feature_size, feature_size)\n",
    "        self.value = nn.Linear(feature_size, feature_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Apply linear transformations\n",
    "        keys = self.key(x)\n",
    "        queries = self.query(x)\n",
    "        values = self.value(x)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.feature_size, dtype=torch.float32))\n",
    "\n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Multiply weights with values\n",
    "        output = torch.matmul(attention_weights, values)\n",
    "\n",
    "        return output\n",
    "class IntentClassifierLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(IntentClassifierLSTMWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = SelfAttentionLayer(hidden_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        dropped = self.dropout(embedded)\n",
    "        lstm_out, _ = self.lstm(dropped)\n",
    "\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(lstm_out)\n",
    "        final_output= attn_out[:, -1, :]\n",
    "        normalized = self.batch_norm(final_output)\n",
    "\n",
    "        out = self.fc(normalized)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af8efc5-d23a-46fe-acdf-eb4f7087d0f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:34:41.644291Z",
     "start_time": "2023-11-21T02:34:41.636748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate=0.001\n",
    "weight_decay=1e-4\n",
    "dropout_rate=0.4\n",
    "embedding_dim =64            # Size of each embedding vector\n",
    "hidden_dim = 128               # Number of features in the hidden state of the LSTM\n",
    "batch_size = 32\n",
    "output_dim = len(le.classes_)  # Number of classes\n",
    "num_epochs=20\n",
    "# Create a string that summarizes these parameters\n",
    "params_str = f\"Vocab Size: {vocab_size}\\n\" \\\n",
    "             f\"Embedding Dim: {embedding_dim}\\n\" \\\n",
    "             f\"Hidden Dim: {hidden_dim}\\n\" \\\n",
    "             f\"Output Dim: {output_dim}\\n\" \\\n",
    "             f\"Dropout Rate: {dropout_rate}\\n\" \\\n",
    "             f\"learning Rate: {learning_rate}\\n\" \\\n",
    "             f\"epochs: {num_epochs}\"\n",
    "print(params_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd27564",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Training and Evaluation\n",
    "Implement a training loop for the model. You can use any optimizer and loss function of your choice. You can also use any other metric that you think is suitable for the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e71e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:34:42.644576Z",
     "start_time": "2023-11-21T02:34:42.639088Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "device=\"mps\"\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5a36b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:37:08.084518Z",
     "start_time": "2023-11-21T02:36:27.306232Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "#model = IntentClassifierLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "model = IntentClassifierLSTMWithAttention(vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    correct=0.0\n",
    "    acc=0\n",
    "    for batch in train_loader:\n",
    "        # get data\n",
    "        x, y = batch\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        y_hat = model(x)\n",
    "        # compute loss\n",
    "        loss = loss_function(y_hat, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # step\n",
    "        optimizer.step()\n",
    "        # update train loss\n",
    "        # compute accuracy\n",
    "        _, predicted = torch.max(y_hat, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "    # compute average losses\n",
    "    train_loss /= len(train_loader)\n",
    "    acc=(correct/len(train_padded))\n",
    "\n",
    "    # log average losses\n",
    "    mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "    # Log training loss per epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521bdb41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:37:16.367608Z",
     "start_time": "2023-11-21T02:37:14.259113Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# evaluate on test set\n",
    "test_loss = 0.0\n",
    "correct=0\n",
    "acc=0\n",
    "for batch in test_loader:\n",
    "    # get data\n",
    "    x, y = batch\n",
    "    # forward pass\n",
    "    y_hat = model(x)\n",
    "    # compute loss\n",
    "    loss = loss_function(y_hat, y)\n",
    "    # update test loss\n",
    "    test_loss += loss.item()\n",
    "    _, predicted = torch.max(y_hat, 1)\n",
    "    correct += (predicted == y).sum().item()\n",
    "# compute average losses\n",
    "test_loss /= len(test_loader)\n",
    "acc=(correct/len(test_padded))\n",
    "\n",
    "# log average losses\n",
    "mlflow.log_metric(\"test_loss\", test_loss, step=epoch)\n",
    "mlflow.log_metric(\"test_accuracy\", acc, step=epoch)\n",
    "# Log the precision and recall\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# log model\n",
    "mlflow.pytorch.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a3193f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:36:12.804884Z",
     "start_time": "2023-11-21T02:35:41.253009Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    pass\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "#model = IntentClassifierLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "model = IntentClassifierLSTMWithAttention(vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Convert labels to torch tensor\n",
    "train_labels_tensor = torch.tensor(train_labels).to(device)\n",
    "test_labels_tensor = torch.tensor(test_labels).to(device)\n",
    "print(train_labels_tensor)\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter()\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_artifacts(writer.log_dir, artifact_path=\"tensorboard_logs\")\n",
    "\n",
    "    # Log model architecture\n",
    "    mlflow.log_text(str(model), \"model.txt\")\n",
    "\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_text(params_str, \"model_parameters.txt\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss=0\n",
    "        correct=0\n",
    "        accuracy=0\n",
    "        for i in range(0, len(train_padded), batch_size):\n",
    "            # Batch inputs and labels\n",
    "            input_batch = train_padded[i:i+batch_size].to(device)\n",
    "            label_batch = train_labels_tensor[i:i+batch_size]\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_batch)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_function(outputs, label_batch)\n",
    "            train_loss+=loss.item()\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == label_batch).sum().item()\n",
    "        avg_loss=train_loss/len(train_padded)\n",
    "        accuracy=(correct/len(train_padded))\n",
    "        # Log training loss per epoch\n",
    "        mlflow.log_metric(\"train_loss\",avg_loss , step=epoch)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.4f}')\n",
    "        if accuracy>0.995:\n",
    "            break\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    correct=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_padded), batch_size):\n",
    "            # Batch inputs and labels\n",
    "            input_batch = test_padded[i:i+batch_size]\n",
    "            label_batch = test_labels_tensor[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == label_batch).sum().item()\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, label_batch)\n",
    "            total_loss += loss.item()\n",
    "            #total_loss += loss.item() * input_batch.size(0)\n",
    "\n",
    "\n",
    "    # Compute and log the average test loss\n",
    "    average_loss = total_loss / len(test_padded)\n",
    "    print(f\"Test Loss: {average_loss:.4f}\")\n",
    "    accuracy = (correct / len(test_padded))\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    mlflow.log_metric(\"test_loss\", average_loss)\n",
    "    mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "    writer.add_scalar(\"Loss/test\", average_loss)\n",
    "\n",
    "    # Log the final model to MLflow\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f57a31-f615-4295-a536-4f002f30d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "#model = IntentClassifierLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "model = IntentClassifierLSTMWithAttention(vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Convert labels to torch tensor\n",
    "train_labels_tensor = torch.tensor(train_labels).to(device)\n",
    "test_labels_tensor = torch.tensor(test_labels).to(device)\n",
    "print(train_labels_tensor)\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter()\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_artifacts(writer.log_dir, artifact_path=\"tensorboard_logs\")\n",
    "\n",
    "    # Log model architecture\n",
    "    mlflow.log_text(str(model), \"model.txt\")\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    mlflow.log_text(params_str, \"model_parameters.txt\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss=0\n",
    "        correct=0\n",
    "        accuracy=0\n",
    "        for i in range(0, len(train_padded), batch_size):\n",
    "            # Batch inputs and labels\n",
    "            input_batch = train_padded[i:i+batch_size].to(device)\n",
    "            label_batch = train_labels_tensor[i:i+batch_size]\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_batch)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_function(outputs, label_batch)\n",
    "            train_loss+=loss.item()\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == label_batch).sum().item()\n",
    "        avg_loss=train_loss/len(train_padded)\n",
    "        accuracy=(correct/len(train_padded))\n",
    "        # Log training loss per epoch\n",
    "        mlflow.log_metric(\"train_loss\",avg_loss , step=epoch)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.4f}')\n",
    "        if accuracy>0.998:\n",
    "            break\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    correct=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_padded), batch_size):\n",
    "            # Batch inputs and labels\n",
    "            input_batch = test_padded[i:i+batch_size]\n",
    "            label_batch = test_labels_tensor[i:i+batch_size]\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(input_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == label_batch).sum().item()\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, label_batch)\n",
    "            total_loss += loss.item()\n",
    "            #total_loss += loss.item() * input_batch.size(0)\n",
    "      \n",
    "\n",
    "    # Compute and log the average test loss\n",
    "    average_loss = total_loss / len(test_padded)\n",
    "    print(f\"Test Loss: {average_loss:.4f}\")\n",
    "    accuracy = (correct / len(test_padded))\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    mlflow.log_metric(\"test_loss\", average_loss)\n",
    "    writer.add_scalar(\"Loss/test\", average_loss)\n",
    "\n",
    "    # Log the final model to MLflow\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d483f-80b5-44a2-8f6c-e3f4169f37d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:37:35.823573Z",
     "start_time": "2023-11-21T02:37:35.719218Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_padded), batch_size):\n",
    "        # Batch inputs and labels\n",
    "        input_batch = test_padded[i:i+batch_size].to(device)\n",
    "        label_batch = test_labels_tensor[i:i+batch_size].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Convert predictions and labels to CPU and then to NumPy\n",
    "        predicted_np = predicted.cpu().numpy()\n",
    "        labels_np = label_batch.cpu().numpy()\n",
    "\n",
    "        # Convert numerical labels to original categorical labels\n",
    "        predicted_labels = le.inverse_transform(predicted_np)\n",
    "        actual_labels = le.inverse_transform(labels_np)\n",
    "        '''\n",
    "        # Print predicted and actual labels side by side\n",
    "        for pred, actual in zip(predicted_labels, actual_labels):\n",
    "            print(f\"Predicted: {pred}, Actual: {actual}\")\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134ba01-9415-4e6a-a524-2e1e1d4ac724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:37:49.448139Z",
     "start_time": "2023-11-21T02:37:49.442239Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')\n",
    "#model_serve=IntentClassifierLSTMWithAttention(cfg.vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "#model_serve.load_state_dict(torch.load('model_state_dict.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ccd1f-03f0-43ea-b23c-390b9bbba0ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T02:37:55.878326Z",
     "start_time": "2023-11-21T02:37:55.151694Z"
    }
   },
   "outputs": [],
   "source": [
    "model_serve = torch.load('model.pth')\n",
    "\n",
    "def predict(model, query, max_length):\n",
    "    model.eval()\n",
    "    # Tokenize and prepare input\n",
    "    query_indices = [text_to_indices(text, word_to_index) for text in query]\n",
    "    print(query_indices)\n",
    "    query_tensor = [torch.tensor(seq).to(device) for seq in query_indices]\n",
    "    print(query_tensor)\n",
    "    input = pad_sequence(query_tensor, batch_first=True, padding_value=0)\n",
    "    print(input)\n",
    "    # Inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Convert prediction to label\n",
    "    return le.inverse_transform(predicted.data.cpu().numpy())\n",
    "torch.save(model.state_dict(), 'model_state_dict.pth')\n",
    "\n",
    "\n",
    "\n",
    "query=list()\n",
    "query.append(\"what airlines off from love field between 6 and 10 am on june sixth\")\n",
    "query.append(\"unknown sequence\")\n",
    "prediction = predict(model_serve, query,46)\n",
    "print(f\"Predicted label: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda57e3d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultimate_nlp",
   "language": "python",
   "name": "ultimate-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
