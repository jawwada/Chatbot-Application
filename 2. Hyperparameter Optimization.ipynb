{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#    Hyperparameter Tuning for Intent Classification using LSTM with Attention Mechanism\n",
    "This notebook is 2nd part of the Ultimate AI Challenge. In the previous notebook, I trained a LSTM Model with Attention Mechanism for Intent Classification on the ATIS dataset. In this notebook, I perform a hyper parameter optimization to learn the best parameters for the model and also log our experiments using an ML server, e.g. MLFlow. Propreitary clouds already use experiment logging, model registry and deployment services, e.g. Vertex AI in GCP, Azure ML and Amazon Sagemaker. I have worked in Vertex AI and Azure ML. MLflow + Optuna is a good open source and scalable alternative to them. I use Optuna for hyperparameter tuning and MLflow for experiment tracking and model registry. Model Serving is done through pytorch save and load methods. To avoid pickling and version issues, one can also use model state dictionaries. The notebook is divided into the following sections:\n",
    "1. Data Preparation\n",
    "2. Hyperparameter Tuning\n",
    "3. Model Training and Evaluation\n",
    "4. Visualize the results\n",
    "5. Model Export, Registry, Model Loading and Inference\n",
    "\n",
    "Let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T17:00:41.055790Z",
     "start_time": "2023-11-23T17:00:34.092485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn as nn\n",
    "from machine_learning.IntentClassifierLSTMWithAttention import IntentClassifierLSTMWithAttention\n",
    "from machine_learning.IntentTokenizer import IntentTokenizer\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from machine_learning.model_utils import train, evaluate, predict, get_or_create_experiment\n",
    "import optuna\n",
    "import logging\n",
    "import mlflow\n",
    "from optuna.visualization import plot_optimization_history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Preparation\n",
    "Steps:\n",
    "1. Load the data\n",
    "2. Tokenize the data\n",
    "3. Create a PyTorch Dataset\n",
    "4. Create a PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T17:03:18.946528Z",
     "start_time": "2023-11-23T17:03:18.934354Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside IntentTokenizer\n",
      "Actual Vocabulary Size: 890\n",
      "Encoding labels for the first time and adding unknown class.\n",
      "Label Encoding: {'abbreviation': 0, 'aircraft': 1, 'aircraft+flight+flight_no': 2, 'airfare': 3, 'airfare+flight_time': 4, 'airline': 5, 'airline+flight_no': 6, 'airport': 7, 'capacity': 8, 'cheapest': 9, 'city': 10, 'distance': 11, 'flight': 12, 'flight+airfare': 13, 'flight_no': 14, 'flight_time': 15, 'ground_fare': 16, 'ground_service': 17, 'ground_service+ground_fare': 18, 'meal': 19, 'quantity': 20, 'restriction': 21, '<unknown>': 22}\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "train_df = pd.read_csv('data/atis/train.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "test_df = pd.read_csv('data/atis/test.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "tokenizer = IntentTokenizer(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T17:02:13.698118Z",
     "start_time": "2023-11-23T17:02:13.669268Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: torch.Size([4634, 46])\n",
      "Number of training batches: 145\n",
      "Number of test samples: torch.Size([850, 30])\n",
      "Number of test batches: 27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define constants and hyperparameters\n",
    "vocab_size=tokenizer.max_vocab_size\n",
    "output_dim=len(tokenizer.le.classes_)\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "\n",
    "train_data = tokenizer.process_data(train_df, device=device)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "print(\"Number of training samples:\", train_data.tensors[0].size())\n",
    "print(\"Number of training batches:\", len(train_loader))\n",
    "\n",
    "test_data = tokenizer.process_data(test_df, device=device)\n",
    "print(\"Number of test samples:\", test_data.tensors[0].size())\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "print(\"Number of test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "The Model I am using for the hyperparameter tuning is the IntentClassifierLSTMWithAttention. The hyperparameters are:\n",
    "1. Learning rate : 1e-3 to 1e-1\n",
    "2. Hidden dimension : 32, 64, 128, 256\n",
    "3. Embedding dimension : 64, 128, 256, 512\n",
    "4. Dropout rate : 0.1 to 0.5\n",
    "5. Weight decay : 1e-6 to 1e-3\n",
    "\n",
    "The objective function is the average validation accuracy over 5 folds. The best model is the one with the highest average validation accuracy. Note our test data is completely hidden to the accuracy optimizer of optuna. One can choose any metric to optimize, for example minimization of loss, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def log_hyperparameters(trial):\n",
    "    # Log hyperparameters\n",
    "    \n",
    "    mlflow.log_param(\"lr\", trial.params[\"lr\"])\n",
    "    mlflow.log_param(\"hidden_dim\", trial.params[\"hidden_dim\"])\n",
    "    mlflow.log_param(\"embedding_dim\", trial.params[\"embedding_dim\"])\n",
    "    mlflow.log_param(\"dropout_rate\", trial.params[\"dropout_rate\"])\n",
    "    mlflow.log_param(\"weight_decay\", trial.params[\"weight_decay\"])\n",
    "    print(f'lr: {trial.params[\"lr\"]}, hidden_dim: {trial.params[\"hidden_dim\"]}, embedding_dim: {trial.params[\"embedding_dim\"]}, dropout_rate: {trial.params[\"dropout_rate\"]}, weight_decay: {trial.params[\"weight_decay\"]}')\n",
    "\n",
    "    return\n",
    "\n",
    "def log_metrics(trial, accuracy):\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    return\n",
    "def objective(trial):\n",
    "    with mlflow.start_run():\n",
    "        # Suggest hyperparameters\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [32, 64, 256])\n",
    "        embedding_dim = trial.suggest_categorical(\"embedding_dim\", [64, 128, 256])\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        log_hyperparameters(trial)\n",
    "        # Model, loss, and optimizer\n",
    "        # model = IntentClassifierLSTM(cfg.vocab_size, embedding_dim, hidden_dim, cfg.output_dim,dropout_rate).to(device)\n",
    "        model = IntentClassifierLSTMWithAttention(vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_val_acc = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(train_df)):\n",
    "            # Prepare fold data\n",
    "            train_data_subset = tokenizer.process_data(train_df.loc[train_idx,:], device=device)\n",
    "            val_data_subset = tokenizer.process_data(train_df.loc[val_idx,:], device=device)\n",
    "            train_subset_loader = DataLoader(train_data_subset, batch_size=batch_size, shuffle=True)\n",
    "            val_subset_loader = DataLoader(val_data_subset, batch_size=batch_size, shuffle=False)\n",
    "            fold_loss = train(model, optimizer, criterion, train_subset_loader, num_epochs)\n",
    "            val_accuracy = evaluate(model,  criterion, val_subset_loader, data_type=\"Validation\")\n",
    "            print(f'Fold: {fold + 1}, Training Loss: {fold_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "            fold_val_acc.append(val_accuracy)\n",
    "        average_val_acc = sum(fold_val_acc) / len(fold_val_acc)\n",
    "        print(f'Average validation accuracy: {average_val_acc:.4f}')\n",
    "        log_metrics(trial, average_val_acc)\n",
    "    return average_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Optuna Study for Hyperparameter Optimization\n",
    "Create an experiment in MLflow and run the hyperparameter tuning experiment. The best model is the one with the highest average validation accuracy. You can install the mlflow and optuna using pip install. To start mlflow server, I used the following command\n",
    "`mlflow server --backend-store-uri=sqlite:///mlrunsdb15.db --default-artifact-root=file:mlruns --host 127.0.0.1 --port 1234`\n",
    "Once started, it creates mlruns directory in project root and also a db in the root to log the progress, store models and experiments. In practise, it is installed on a server, and http requests are sent to the server. Anyhow pay close attention to the directory you give in the command to run the server. Here I keep it simple. Optuna also creates a db to store it's studies. Here I choose 20 tries. For 5 folds cross validation, and 5 epochs each, the study takes less than an hour my M1 machine. Optuna uses Bayesian Parameter Optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the MLflow Server\n",
    "\n",
    "pip install mlflow\n",
    "\n",
    "`mlflow server --backend-store-uri=mlruns --default-artifact-root=file:mlruns --host 127.0.0.1 --port 1234`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T17:00:45.585347Z",
     "start_time": "2023-11-23T17:00:45.547671Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001136405820053151, hidden_dim: 64, embedding_dim: 128, dropout_rate: 0.25433405543574206, weight_decay: 0.0008797794555903052\n",
      "Epoch [1/5], Loss: 1.7839, Accuracy: 0.6593\n",
      "Epoch [2/5], Loss: 0.5340, Accuracy: 0.8999\n",
      "Epoch [3/5], Loss: 0.3303, Accuracy: 0.9304\n",
      "Epoch [4/5], Loss: 0.2368, Accuracy: 0.9463\n",
      "Epoch [5/5], Loss: 0.1746, Accuracy: 0.9587\n",
      "Validation Loss: 0.1229\n",
      "Validation Accuracy: 0.9687\n",
      "Fold: 1, Training Loss: 0.1746, Validation Accuracy: 0.9687\n",
      "Epoch [1/5], Loss: 0.1454, Accuracy: 0.9655\n",
      "Epoch [2/5], Loss: 0.1238, Accuracy: 0.9709\n",
      "Epoch [3/5], Loss: 0.1063, Accuracy: 0.9752\n",
      "Epoch [4/5], Loss: 0.0930, Accuracy: 0.9771\n",
      "Epoch [5/5], Loss: 0.0755, Accuracy: 0.9811\n",
      "Validation Loss: 0.1239\n",
      "Validation Accuracy: 0.9741\n",
      "Fold: 2, Training Loss: 0.0755, Validation Accuracy: 0.9741\n",
      "Epoch [1/5], Loss: 0.0963, Accuracy: 0.9760\n",
      "Epoch [2/5], Loss: 0.0799, Accuracy: 0.9790\n",
      "Epoch [3/5], Loss: 0.0802, Accuracy: 0.9800\n",
      "Epoch [4/5], Loss: 0.0638, Accuracy: 0.9844\n",
      "Epoch [5/5], Loss: 0.0710, Accuracy: 0.9825\n",
      "Validation Loss: 0.0570\n",
      "Validation Accuracy: 0.9838\n",
      "Fold: 3, Training Loss: 0.0710, Validation Accuracy: 0.9838\n",
      "Epoch [1/5], Loss: 0.0658, Accuracy: 0.9825\n",
      "Epoch [2/5], Loss: 0.0544, Accuracy: 0.9871\n",
      "Epoch [3/5], Loss: 0.0505, Accuracy: 0.9895\n",
      "Epoch [4/5], Loss: 0.0598, Accuracy: 0.9817\n",
      "Epoch [5/5], Loss: 0.0434, Accuracy: 0.9895\n",
      "Validation Loss: 0.0556\n",
      "Validation Accuracy: 0.9838\n",
      "Fold: 4, Training Loss: 0.0434, Validation Accuracy: 0.9838\n",
      "Epoch [1/5], Loss: 0.0720, Accuracy: 0.9809\n",
      "Epoch [2/5], Loss: 0.0540, Accuracy: 0.9838\n",
      "Epoch [3/5], Loss: 0.0440, Accuracy: 0.9884\n",
      "Epoch [4/5], Loss: 0.0427, Accuracy: 0.9889\n",
      "Epoch [5/5], Loss: 0.0422, Accuracy: 0.9873\n",
      "Validation Loss: 0.0318\n",
      "Validation Accuracy: 0.9892\n",
      "Fold: 5, Training Loss: 0.0422, Validation Accuracy: 0.9892\n",
      "Average validation accuracy: 0.9799\n",
      "lr: 0.00012043340031232762, hidden_dim: 64, embedding_dim: 64, dropout_rate: 0.3472802726775268, weight_decay: 0.00010934384273914595\n",
      "Epoch [1/5], Loss: 3.1553, Accuracy: 0.1389\n",
      "Epoch [2/5], Loss: 2.7031, Accuracy: 0.4157\n",
      "Epoch [3/5], Loss: 2.2270, Accuracy: 0.6070\n",
      "Epoch [4/5], Loss: 1.8934, Accuracy: 0.6776\n",
      "Epoch [5/5], Loss: 1.6168, Accuracy: 0.7354\n",
      "Validation Loss: 0.8960\n",
      "Validation Accuracy: 0.8576\n",
      "Fold: 1, Training Loss: 1.6168, Validation Accuracy: 0.8576\n",
      "Epoch [1/5], Loss: 1.3513, Accuracy: 0.7837\n",
      "Epoch [2/5], Loss: 1.1243, Accuracy: 0.8265\n",
      "Epoch [3/5], Loss: 0.9996, Accuracy: 0.8381\n",
      "Epoch [4/5], Loss: 0.8647, Accuracy: 0.8549\n",
      "Epoch [5/5], Loss: 0.7640, Accuracy: 0.8659\n",
      "Validation Loss: 0.5257\n",
      "Validation Accuracy: 0.8954\n",
      "Fold: 2, Training Loss: 0.7640, Validation Accuracy: 0.8954\n",
      "Epoch [1/5], Loss: 0.7066, Accuracy: 0.8808\n",
      "Epoch [2/5], Loss: 0.6826, Accuracy: 0.8805\n",
      "Epoch [3/5], Loss: 0.5958, Accuracy: 0.8859\n",
      "Epoch [4/5], Loss: 0.5404, Accuracy: 0.8940\n",
      "Epoch [5/5], Loss: 0.5314, Accuracy: 0.8956\n",
      "Validation Loss: 0.3627\n",
      "Validation Accuracy: 0.9169\n",
      "Fold: 3, Training Loss: 0.5314, Validation Accuracy: 0.9169\n",
      "Epoch [1/5], Loss: 0.4885, Accuracy: 0.9061\n",
      "Epoch [2/5], Loss: 0.4552, Accuracy: 0.9053\n",
      "Epoch [3/5], Loss: 0.4490, Accuracy: 0.9072\n",
      "Epoch [4/5], Loss: 0.4473, Accuracy: 0.9064\n",
      "Epoch [5/5], Loss: 0.4174, Accuracy: 0.9115\n",
      "Validation Loss: 0.3135\n",
      "Validation Accuracy: 0.9234\n",
      "Fold: 4, Training Loss: 0.4174, Validation Accuracy: 0.9234\n",
      "Epoch [1/5], Loss: 0.3875, Accuracy: 0.9183\n",
      "Epoch [2/5], Loss: 0.3768, Accuracy: 0.9175\n",
      "Epoch [3/5], Loss: 0.3742, Accuracy: 0.9196\n",
      "Epoch [4/5], Loss: 0.3474, Accuracy: 0.9242\n",
      "Epoch [5/5], Loss: 0.3401, Accuracy: 0.9188\n",
      "Validation Loss: 0.2092\n",
      "Validation Accuracy: 0.9525\n",
      "Fold: 5, Training Loss: 0.3401, Validation Accuracy: 0.9525\n",
      "Average validation accuracy: 0.9092\n",
      "lr: 0.0019424396204877775, hidden_dim: 32, embedding_dim: 64, dropout_rate: 0.47627930166463994, weight_decay: 3.802594010691171e-05\n",
      "Epoch [1/5], Loss: 1.9837, Accuracy: 0.5956\n",
      "Epoch [2/5], Loss: 0.7039, Accuracy: 0.8465\n",
      "Epoch [3/5], Loss: 0.4719, Accuracy: 0.8862\n",
      "Epoch [4/5], Loss: 0.3793, Accuracy: 0.9059\n",
      "Epoch [5/5], Loss: 0.3303, Accuracy: 0.9150\n",
      "Validation Loss: 0.1724\n",
      "Validation Accuracy: 0.9547\n",
      "Fold: 1, Training Loss: 0.3303, Validation Accuracy: 0.9547\n",
      "Epoch [1/5], Loss: 0.2621, Accuracy: 0.9301\n",
      "Epoch [2/5], Loss: 0.2274, Accuracy: 0.9390\n",
      "Epoch [3/5], Loss: 0.1884, Accuracy: 0.9463\n",
      "Epoch [4/5], Loss: 0.1728, Accuracy: 0.9533\n",
      "Epoch [5/5], Loss: 0.1514, Accuracy: 0.9585\n",
      "Validation Loss: 0.1576\n",
      "Validation Accuracy: 0.9644\n",
      "Fold: 2, Training Loss: 0.1514, Validation Accuracy: 0.9644\n",
      "Epoch [1/5], Loss: 0.1719, Accuracy: 0.9512\n",
      "Epoch [2/5], Loss: 0.1415, Accuracy: 0.9625\n",
      "Epoch [3/5], Loss: 0.1273, Accuracy: 0.9639\n",
      "Epoch [4/5], Loss: 0.1093, Accuracy: 0.9717\n",
      "Epoch [5/5], Loss: 0.1050, Accuracy: 0.9714\n",
      "Validation Loss: 0.0512\n",
      "Validation Accuracy: 0.9838\n",
      "Fold: 3, Training Loss: 0.1050, Validation Accuracy: 0.9838\n",
      "Epoch [1/5], Loss: 0.1024, Accuracy: 0.9722\n",
      "Epoch [2/5], Loss: 0.0853, Accuracy: 0.9749\n",
      "Epoch [3/5], Loss: 0.0837, Accuracy: 0.9744\n",
      "Epoch [4/5], Loss: 0.0997, Accuracy: 0.9711\n",
      "Epoch [5/5], Loss: 0.0854, Accuracy: 0.9755\n",
      "Validation Loss: 0.0522\n",
      "Validation Accuracy: 0.9838\n",
      "Fold: 4, Training Loss: 0.0854, Validation Accuracy: 0.9838\n",
      "Epoch [1/5], Loss: 0.0730, Accuracy: 0.9803\n",
      "Epoch [2/5], Loss: 0.0628, Accuracy: 0.9811\n",
      "Epoch [3/5], Loss: 0.0585, Accuracy: 0.9833\n",
      "Epoch [4/5], Loss: 0.0708, Accuracy: 0.9782\n",
      "Epoch [5/5], Loss: 0.0613, Accuracy: 0.9830\n",
      "Validation Loss: 0.0331\n",
      "Validation Accuracy: 0.9914\n",
      "Fold: 5, Training Loss: 0.0613, Validation Accuracy: 0.9914\n",
      "Average validation accuracy: 0.9756\n",
      "lr: 0.008782640986087536, hidden_dim: 32, embedding_dim: 128, dropout_rate: 0.25994849817442917, weight_decay: 0.00048111206281877753\n",
      "Epoch [1/5], Loss: 0.9633, Accuracy: 0.7963\n",
      "Epoch [2/5], Loss: 0.2672, Accuracy: 0.9334\n",
      "Epoch [3/5], Loss: 0.1703, Accuracy: 0.9541\n",
      "Epoch [4/5], Loss: 0.1173, Accuracy: 0.9676\n",
      "Epoch [5/5], Loss: 0.1282, Accuracy: 0.9674\n",
      "Validation Loss: 0.1231\n",
      "Validation Accuracy: 0.9687\n",
      "Fold: 1, Training Loss: 0.1282, Validation Accuracy: 0.9687\n",
      "Epoch [1/5], Loss: 0.1171, Accuracy: 0.9698\n",
      "Epoch [2/5], Loss: 0.0997, Accuracy: 0.9725\n",
      "Epoch [3/5], Loss: 0.0707, Accuracy: 0.9811\n",
      "Epoch [4/5], Loss: 0.0660, Accuracy: 0.9825\n",
      "Epoch [5/5], Loss: 0.0992, Accuracy: 0.9749\n",
      "Validation Loss: 0.1429\n",
      "Validation Accuracy: 0.9579\n",
      "Fold: 2, Training Loss: 0.0992, Validation Accuracy: 0.9579\n",
      "Epoch [1/5], Loss: 0.0985, Accuracy: 0.9752\n",
      "Epoch [2/5], Loss: 0.0918, Accuracy: 0.9760\n",
      "Epoch [3/5], Loss: 0.0715, Accuracy: 0.9808\n",
      "Epoch [4/5], Loss: 0.0809, Accuracy: 0.9792\n",
      "Epoch [5/5], Loss: 0.0695, Accuracy: 0.9800\n",
      "Validation Loss: 0.1031\n",
      "Validation Accuracy: 0.9709\n",
      "Fold: 3, Training Loss: 0.0695, Validation Accuracy: 0.9709\n",
      "Epoch [1/5], Loss: 0.0890, Accuracy: 0.9760\n",
      "Epoch [2/5], Loss: 0.0734, Accuracy: 0.9817\n",
      "Epoch [3/5], Loss: 0.0641, Accuracy: 0.9825\n",
      "Epoch [4/5], Loss: 0.0682, Accuracy: 0.9833\n",
      "Epoch [5/5], Loss: 0.0744, Accuracy: 0.9830\n",
      "Validation Loss: 0.1052\n",
      "Validation Accuracy: 0.9709\n",
      "Fold: 4, Training Loss: 0.0744, Validation Accuracy: 0.9709\n",
      "Epoch [1/5], Loss: 0.0730, Accuracy: 0.9822\n",
      "Epoch [2/5], Loss: 0.0614, Accuracy: 0.9841\n",
      "Epoch [3/5], Loss: 0.0923, Accuracy: 0.9757\n",
      "Epoch [4/5], Loss: 0.0635, Accuracy: 0.9835\n",
      "Epoch [5/5], Loss: 0.0659, Accuracy: 0.9827\n",
      "Validation Loss: 0.0850\n",
      "Validation Accuracy: 0.9795\n",
      "Fold: 5, Training Loss: 0.0659, Validation Accuracy: 0.9795\n",
      "Average validation accuracy: 0.9696\n",
      "lr: 0.0007504680562042859, hidden_dim: 32, embedding_dim: 128, dropout_rate: 0.4286637993027823, weight_decay: 1.1671613018755621e-05\n",
      "Epoch [1/5], Loss: 2.7309, Accuracy: 0.3480\n",
      "Epoch [2/5], Loss: 1.4832, Accuracy: 0.7556\n",
      "Epoch [3/5], Loss: 0.8822, Accuracy: 0.8346\n",
      "Epoch [4/5], Loss: 0.6441, Accuracy: 0.8697\n",
      "Epoch [5/5], Loss: 0.5157, Accuracy: 0.8937\n",
      "Validation Loss: 0.3537\n",
      "Validation Accuracy: 0.9223\n",
      "Fold: 1, Training Loss: 0.5157, Validation Accuracy: 0.9223\n",
      "Epoch [1/5], Loss: 0.4225, Accuracy: 0.9083\n",
      "Epoch [2/5], Loss: 0.3435, Accuracy: 0.9210\n",
      "Epoch [3/5], Loss: 0.3237, Accuracy: 0.9231\n",
      "Epoch [4/5], Loss: 0.2966, Accuracy: 0.9247\n",
      "Epoch [5/5], Loss: 0.2478, Accuracy: 0.9366\n",
      "Validation Loss: 0.2218\n",
      "Validation Accuracy: 0.9471\n",
      "Fold: 2, Training Loss: 0.2478, Validation Accuracy: 0.9471\n",
      "Epoch [1/5], Loss: 0.2431, Accuracy: 0.9420\n",
      "Epoch [2/5], Loss: 0.2135, Accuracy: 0.9474\n",
      "Epoch [3/5], Loss: 0.2025, Accuracy: 0.9501\n",
      "Epoch [4/5], Loss: 0.1852, Accuracy: 0.9523\n",
      "Epoch [5/5], Loss: 0.1688, Accuracy: 0.9585\n",
      "Validation Loss: 0.1112\n",
      "Validation Accuracy: 0.9698\n",
      "Fold: 3, Training Loss: 0.1688, Validation Accuracy: 0.9698\n",
      "Epoch [1/5], Loss: 0.1569, Accuracy: 0.9590\n",
      "Epoch [2/5], Loss: 0.1483, Accuracy: 0.9622\n",
      "Epoch [3/5], Loss: 0.1263, Accuracy: 0.9668\n",
      "Epoch [4/5], Loss: 0.1165, Accuracy: 0.9703\n",
      "Epoch [5/5], Loss: 0.1283, Accuracy: 0.9633\n",
      "Validation Loss: 0.0790\n",
      "Validation Accuracy: 0.9795\n",
      "Fold: 4, Training Loss: 0.1283, Validation Accuracy: 0.9795\n",
      "Epoch [1/5], Loss: 0.1284, Accuracy: 0.9663\n",
      "Epoch [2/5], Loss: 0.1042, Accuracy: 0.9709\n",
      "Epoch [3/5], Loss: 0.0984, Accuracy: 0.9749\n",
      "Epoch [4/5], Loss: 0.0989, Accuracy: 0.9757\n",
      "Epoch [5/5], Loss: 0.0858, Accuracy: 0.9771\n",
      "Validation Loss: 0.0624\n",
      "Validation Accuracy: 0.9806\n",
      "Fold: 5, Training Loss: 0.0858, Validation Accuracy: 0.9806\n",
      "Average validation accuracy: 0.9599\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:1234')\n",
    "\n",
    "model_class_name = \"IntentClassifierLSTMWithAttention\"\n",
    "#optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "storage_name = \"sqlite:///{}.db\".format(model_class_name)\n",
    "study = optuna.create_study(study_name=model_class_name, load_if_exists=True, storage=storage_name,direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=5)\n",
    "best_trial = study.best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Best Model Training and Evaluation (Saving, Reloading the best model, through optuna or mlflow)\n",
    "Once I find the best set of hyper parameters, you can open mlflow on tracking uri, e.g. in my case , I set http://127.0.0.1:1234. Also if you install optuna-dashboard, you can see very nice visualizations and get very detailed insights on hyperparameter space, their importance, and why some combinations work more than others. I invite to try it. Tune the best model with a larger number of epochs and evaluate it on the test set. You can reload an optuna study from its' db later to get the best parameters and train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Train the best model\n",
    "Let's say that you have saved an optuna study and you want to reload it later. You feel that the number of epochs were not enough and you want to train the best model with more epochs. Or you want save the model initialization parameters so that you can load the model initial state later and use it along with the saved state dictionary. You can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:58:40.222221Z",
     "start_time": "2023-11-23T16:58:39.808658Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import logging\n",
    "import sys\n",
    "# Reload Optuna Study to get the best parameters\n",
    "experiment_id = get_or_create_experiment(model_class_name)\n",
    "\n",
    "mlflow.set_experiment(experiment_id=experiment_id)\n",
    "storage = optuna.storages.RDBStorage(url=f\"sqlite:///{model_class_name}.db\")\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout));\n",
    "study = optuna.create_study(study_name=model_class_name, storage=storage,load_if_exists=True,direction=\"maximize\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:58:40.222221Z",
     "start_time": "2023-11-23T16:58:39.808658Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: score 0.9903,\n",
      "params {'lr': 0.0013487934809448435, 'hidden_dim': 64, 'embedding_dim': 128, 'dropout_rate': 0.17918187700846566, 'weight_decay': 4.75333827188117e-05}\n"
     ]
    }
   ],
   "source": [
    "best_trial = study.best_trial\n",
    "print(f'Best trial: score {best_trial.value:.4f},\\nparams {best_trial.params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:44:38.603611Z",
     "start_time": "2023-11-23T16:43:49.440045Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.2939, Accuracy: 0.7555\n",
      "Epoch [2/20], Loss: 0.3302, Accuracy: 0.9355\n",
      "Epoch [3/20], Loss: 0.1936, Accuracy: 0.9609\n",
      "Epoch [4/20], Loss: 0.1309, Accuracy: 0.9691\n",
      "Epoch [5/20], Loss: 0.0953, Accuracy: 0.9750\n",
      "Epoch [6/20], Loss: 0.0730, Accuracy: 0.9814\n",
      "Epoch [7/20], Loss: 0.0609, Accuracy: 0.9855\n",
      "Epoch [8/20], Loss: 0.0525, Accuracy: 0.9881\n",
      "Epoch [9/20], Loss: 0.0402, Accuracy: 0.9912\n",
      "Epoch [10/20], Loss: 0.0574, Accuracy: 0.9879\n",
      "Epoch [11/20], Loss: 0.0310, Accuracy: 0.9931\n",
      "Epoch [12/20], Loss: 0.0182, Accuracy: 0.9950\n",
      "Epoch [13/20], Loss: 0.0262, Accuracy: 0.9953\n",
      "Epoch [14/20], Loss: 0.0309, Accuracy: 0.9922\n",
      "Epoch [15/20], Loss: 0.0244, Accuracy: 0.9942\n",
      "Epoch [16/20], Loss: 0.0154, Accuracy: 0.9968\n",
      "Epoch [17/20], Loss: 0.0164, Accuracy: 0.9963\n",
      "Epoch [18/20], Loss: 0.0176, Accuracy: 0.9944\n",
      "Epoch [19/20], Loss: 0.0166, Accuracy: 0.9948\n",
      "Epoch [20/20], Loss: 0.0132, Accuracy: 0.9961\n",
      "Test Loss: 0.3526\n",
      "Test Accuracy: 0.9588\n",
      "Test Accuracy: 0.9588\n"
     ]
    }
   ],
   "source": [
    "### Save , Load and Or train the best model further\n",
    "from machine_learning.IntentTokenizer import IntentTokenizer\n",
    "from machine_learning.IntentClassifierLSTMWithAttention import IntentClassifierLSTMWithAttention\n",
    "from machine_learning.model_utils import train, evaluate, predict\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "train_df = pd.read_csv('data/atis/train.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "#ood_df=pd.read_csv('data/atis/ood.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "#train_df=pd.concat([train_df,ood_df])\n",
    "test_df = pd.read_csv('data/atis/test.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "\n",
    "tokenizer = IntentTokenizer(train_df,5000)\n",
    "\n",
    "train_data = tokenizer.process_data(train_df, device=device)\n",
    "test_data = tokenizer.process_data(test_df, device=device)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # define constants and hyperparameters\n",
    "    num_epochs = 20\n",
    "    model = IntentClassifierLSTMWithAttention(\n",
    "        vocab_size=len(tokenizer.word2idx)+1,\n",
    "        embedding_dim=best_trial.params['embedding_dim'],\n",
    "        hidden_dim=best_trial.params['hidden_dim'],\n",
    "        output_dim=len(tokenizer.le.classes_),\n",
    "        dropout_rate=best_trial.params['dropout_rate']\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_trial.params['lr'],\n",
    "                           weight_decay=best_trial.params['weight_decay'])\n",
    "    train_loss = train(model, optimizer, nn.CrossEntropyLoss(), train_loader, num_epochs)\n",
    "    test_accuracy = evaluate(model, nn.CrossEntropyLoss(), test_loader, data_type=\"Test\")\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Save the model\n",
    "Lets' use a clever construct to save the model. The model is saved with its class name. The parameters are saved in order in a json file. Later on this model can be recreated by providing the class name in the constructor and loading the parameters from the json file. The model state dictionary is saved in a separate file. The tokenizer and label encoder are also saved in separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:46:53.999700Z",
     "start_time": "2023-11-23T16:46:53.978686Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_name=IntentClassifierLSTMWithAttention\n"
     ]
    }
   ],
   "source": [
    "class_name=model.__class__.__name__\n",
    "print(f\"class_name={class_name}\")\n",
    "model.save_config_file(f\"config/{class_name}.json\")\n",
    "torch.save(model.state_dict(),f\"models/{class_name}_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Load the model\n",
    "class_name is a string that can be used to create the model. The model state dictionary is loaded from the saved file. The tokenizer and label encoder are also loaded from the saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:52:53.746102Z",
     "start_time": "2023-11-23T16:52:52.334599Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model and tokenizer from saved files\n",
    "from intent_classifier import IntentClassifier\n",
    "class_name=\"IntentClassifierLSTMWithAttention\"\n",
    "model_serve=IntentClassifier(class_name)\n",
    "model_serve.load(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:56:44.396277Z",
     "start_time": "2023-11-23T16:56:44.360321Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'airfare', 'confidence': 0.8687915205955505}, {'label': 'abbreviation', 'confidence': 0.058476563543081284}, {'label': 'flight', 'confidence': 0.04411102458834648}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query_str = \"how much do you charge for a flight to New York\"\n",
    "response = model_serve.predict(query_str)\n",
    "# Creating a DataFrame with the query string\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:57:00.471362Z",
     "start_time": "2023-11-23T16:57:00.428082Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'flight', 'confidence': 0.9997592568397522}, {'label': 'airport', 'confidence': 0.00011068666935898364}, {'label': 'meal', 'confidence': 2.7888061595149338e-05}]\n"
     ]
    }
   ],
   "source": [
    "query_str = \"I want to book a hotel in New York\"\n",
    "response = model_serve.predict(query_str)\n",
    "# Creating a DataFrame with the query string\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Visualize the results\n",
    "Optuna provides a number of visualizations to analyze the results of the hyperparameter tuning experiment. I use these visualizations to analyze the results of the hyperparameter tuning experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_contour(study)\n",
    "fig.update_layout(\n",
    "    width=1600,   # Width of the figure in pixels\n",
    "    height=800   # Height of the figure in pixels\n",
    ")\n",
    "fig.write_image(\"plotly_contours.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Plotly Contours](plotly_contours.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make very nice conclusions about embedding dimensions and hidden dimmesions along with other dimensions and even restart the study from here. For \n",
    "1. Embedding dimension is ideal somewhere between 100 and 300\n",
    "2. Hidden dimension is ideal between 50 50 100\n",
    "3. learning rate between 0.001 and 0.0001.\n",
    "4. dropout rate between 0.3 to 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_parallel_coordinate(study)\n",
    "fig.update_layout(\n",
    "    width=1600,   # Width of the figure in pixels\n",
    "    height=800   # Height of the figure in pixels\n",
    ")\n",
    "fig.write_image(\"plot_parallel_coordinate.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Parallel Coordinates](plot_parallel_coordinate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check Optuna's complete set of visualizations do the following:\n",
    "\n",
    "!pip install optuna-dashboard\n",
    "\n",
    "!optuna-dashboard sqlite:///IntentClassifierLSTMWithAttention.db "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Save the model and check model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"best_ICELSTMAmodel\"\n",
    "torch.save(model, f\"models/{model_name}.pth\")\n",
    "tokenizer.save_state(f\"models/{model_name}_tokenizer.pickle\", f\"models/{model_name}_le.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: ['airline']\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model and tokenizer from saved files\n",
    "model = torch.load(f\"models/{model_name}.pth\").to(device)\n",
    "tokenizer = IntentTokenizer.load_state(IntentTokenizer,f\"models/{model_name}_tokenizer.pickle\", f\"models/{model_name}_le.pickle\")\n",
    "max_query_length = 50\n",
    "query_text = \"what airlines off from love field between 6 and 10 am on june sixth\"\n",
    "query = pd.DataFrame({\"text\": [query_text]})\n",
    "prediction = predict(model, query,tokenizer,device)\n",
    "print(f\"Predicted label: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step for model improvements\n",
    "\n",
    "Visualize the impact of Attentions and Embeddings\n",
    "One can visualize how attention and embeddings are playing but in the interest of time, I will not much further into improvment of training mechanism. I will shift to model evaulation and performance monitoring in production mode in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultimate_nlpv2",
   "language": "python",
   "name": "ultimate-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
