{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51fd1e24",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Third Party Models. Abstraction of Tokenization and Intent Classifier \n",
    "\n",
    "#### This notebook has two parts. \n",
    "1. using Hugging Face Transformer\n",
    "2. Demonstarion of a Transformer like Interface for Models (Own Tokenizer and Model Abstraction), with train, eval and predict functions that can be used to trained any model.\n",
    "\n",
    "## Transformer, Bert's Intent Classification\n",
    "I trained and fine tuned a language model with Attention Mechanism previously. However, I have not made it generalized. Let's use Hugging face transformers. \n",
    "An example Intent Classification model using BERT and HuggingFace Transformers.\n",
    "Steps:\n",
    "1. Load data\n",
    "2. Tokenize data\n",
    "3. Create PyTorch Dataset\n",
    "4. Train model\n",
    "5. Evaluate model\n",
    "6. Save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ba54bca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:25:23.003867Z",
     "start_time": "2023-11-21T23:25:18.629779Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import config_bert as cfg\n",
    "import warnings\n",
    "from transformers import TrainerCallback\n",
    "from machine_learning.model_utils import get_or_create_experiment\n",
    "from machine_learning.IntentTokenizer import IntentTokenizer\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bd028",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " ## Data Loading\n",
    " I will use the pretrained Bert tokenizer. The tokenizer will convert the text into tokens that the model can understand. The model will be trained to classify the intent of the text. I will use the BertForSequenceClassification model, which is a pretrained Bert model with a single linear classification layer on top. This model can be used for sequence classification tasks like ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c74f208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:25:24.884345Z",
     "start_time": "2023-11-21T23:25:23.005060Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the pre-trained model for sequence classification with the number of labels\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(cfg.le.classes_))\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6eb0f1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## PyTorch Dataset\n",
    "The data set uses encodings from tokenizer and labels from label encoder. The data set is then used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6705cecf-4482-45fa-90b0-3d40b639a6ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:25:25.762342Z",
     "start_time": "2023-11-21T23:25:24.945933Z"
    }
   },
   "outputs": [],
   "source": [
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('data/atis/train.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "test_df = pd.read_csv('data/atis/test.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "\n",
    "# Assume the second column is the label and the first column is the text\n",
    "train_texts = train_df[\"text\"].tolist()\n",
    "test_texts = test_df[\"text\"].tolist()\n",
    "\n",
    "# Convert labels to integer (if they are not already)\n",
    "# This might involve using a LabelEncoder as you have categorical labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = cfg.le\n",
    "num_labels = len(cfg.le.classes_)\n",
    "\n",
    "# Tokenize the text and create datasets\n",
    "max_length = 256  # Max length of the text sequence, you might need to adjust this based on your dataset\n",
    "train_dataset = IntentDataset(train_texts, cfg.train_labels, tokenizer, max_length)\n",
    "test_dataset = IntentDataset(test_texts, cfg.test_labels, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7174b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Training\n",
    "Hyperparameters are defined here. The model is trained and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "329965f9-8bad-46aa-b2f3-b1a3ac1630c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:29:58.967429Z",
     "start_time": "2023-11-21T23:25:25.766465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='870' max='870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [870/870 02:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.180400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.137300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.811700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.445600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.313300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.260800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.204700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.087600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.034300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-5,               # strength of weight decay\n",
    "    logging_dir='./logs',  \n",
    "    logging_strategy=\"steps\",  # or \"epoch\"\n",
    "    logging_steps=50,  # Log every 10 steps# directory for storing logs,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "class MLflowLoggingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        # Log metrics with MLflow here\n",
    "        if metrics:\n",
    "            for key, value in metrics.items():\n",
    "                mlflow.log_metric(key, value, step=state.global_step)\n",
    "\n",
    "try:\n",
    "    # Create an experiment and log parameters\n",
    "    mlflow(pytorch=True)\n",
    "    mlflow.start_run()\n",
    "    mlflow.log_param(\"epochs\", training_args.num_train_epochs)\n",
    "    mlflow.log_param(\"batch_size\", training_args.per_device_train_batch_size)\n",
    "    mlflow.log_param(\"learning_rate\", training_args.learning_rate)\n",
    "    mlflow.log_param(\"weight_decay\", training_args.weight_decay)\n",
    "    mlflow.log_param(\"warmup_steps\", training_args.warmup_steps)\n",
    "    mlflow.log_param(\"max_length\", max_length)\n",
    "    mlflow.log_param(\"num_labels\", num_labels)\n",
    "    mlflow.log_param(\"model\", \"bert-base-uncased\")\n",
    "\n",
    "except:\n",
    "    pass\n",
    "#mlflow.log_params(your_params_dict)  # Log any initial parameters\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[MLflowLoggingCallback()]\n",
    ")\n",
    "trainer.train()\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2facb0df",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed9cecb1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.save_model('results/final_bert_evaluated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea53d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " ## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1afde02-e977-4abd-8203-dc873dbe788f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.22830472886562347,\n",
       " 'eval_runtime': 1.0164,\n",
       " 'eval_samples_per_second': 836.246,\n",
       " 'eval_steps_per_second': 13.773,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83612db-b574-429c-9e79-6e86289c48c8",
   "metadata": {},
   "source": [
    "# Building a generic Hugging Face like Interface\n",
    "Hugging face has its own tokenizer and training interface that abstracts pytorch implementation. I show a similar approach. Classes are implemented in machine_learning directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a563561-1604-4bd0-a45a-2e4d8e521676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from machine_learning.IntentTokenizer import IntentTokenizer\n",
    "from machine_learning.IntentClassifierLSTMWithAttention import IntentClassifierLSTMWithAttention\n",
    "from machine_learning.model_utils import train, evaluate, predict\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load and preprocess the data\n",
    "train_df = pd.read_csv('data/atis/train.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])\n",
    "test_df = pd.read_csv('data/atis/test.tsv', sep='\\t', header=None, names=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e57b0e-2bcf-4f28-842d-71b833b5a1ba",
   "metadata": {},
   "source": [
    "### Own Tokenizer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d9fa6a0-342b-4747-a2f1-b3e0b821deda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside IntentTokenizer\n",
      "Vocabulary Size: 890\n",
      "Encoding labels for the first time and adding unknown class.\n",
      "Label Encoding: {'abbreviation': 0, 'aircraft': 1, 'aircraft+flight+flight_no': 2, 'airfare': 3, 'airfare+flight_time': 4, 'airline': 5, 'airline+flight_no': 6, 'airport': 7, 'capacity': 8, 'cheapest': 9, 'city': 10, 'distance': 11, 'flight': 12, 'flight+airfare': 13, 'flight_no': 14, 'flight_time': 15, 'ground_fare': 16, 'ground_service': 17, 'ground_service+ground_fare': 18, 'meal': 19, 'quantity': 20, 'restriction': 21, '<unknown>': 22}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = IntentTokenizer(train_df)\n",
    "tokenizer.save_state(\"models/IntentClassifierLSTMWithAttention_tokenizer.pickle\", \"models/IntentClassifierLSTMWithAttention_le.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777fff00-ec4f-4208-8893-7ce6f0dad133",
   "metadata": {},
   "source": [
    "### Get data Tensors and Loaders in One go\n",
    "\n",
    "I use a Tupled Tensor Data Set, (two Tensors) first one giving the sequences, and and the 2nd one the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8331cd3e-861f-4575-95df-0944c07ed696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: torch.Size([4634, 46])\n",
      "Number of test samples: torch.Size([850, 30])\n",
      "Number of training batches: 145\n",
      "Number of test batches: 27\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "train_data = tokenizer.process_data(train_df,device=device)\n",
    "test_data = tokenizer.process_data(test_df,device=device)\n",
    "print(\"Number of training samples:\", train_data.tensors[0].size())\n",
    "print(\"Number of test samples:\", test_data.tensors[0].size())\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "print(\"Number of training batches:\", len(train_loader))\n",
    "print(\"Number of test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c693f5-3869-4512-9156-ad44fa88b471",
   "metadata": {},
   "source": [
    "### Encode Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00421fd7-9909-4da8-86d4-d6dbca069d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 891\n",
      "Embedding Dim: 64\n",
      "Hidden Dim: 128\n",
      "Output Dim: 23\n",
      "Dropout Rate: 0.3\n",
      "learning Rate: 0.01\n",
      "epochs: 5\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01              # If you set this too high, it might explode. If too low, it might not learn\n",
    "weight_decay = 1e-7               # Regularization strength\n",
    "dropout_rate = 0.3                 # Dropout rate\n",
    "embedding_dim = 64                # Size of each embedding vector\n",
    "hidden_dim = 128                 # Number of features in the hidden state of the LSTM\n",
    "batch_size = 32                  # Number of samples in each batch\n",
    "output_dim = len(IntentTokenizer.le.classes_)  # Number of classes\n",
    "num_epochs = 5            # Number of times to go through the entire dataset\n",
    "vocab_size = tokenizer.max_vocab_size + 1  # The size of the vocabulary\n",
    "# Create a string that summarizes these parameters\n",
    "params_str = f\"Vocab Size: {vocab_size}\\n\" \\\n",
    "             f\"Embedding Dim: {embedding_dim}\\n\" \\\n",
    "             f\"Hidden Dim: {hidden_dim}\\n\" \\\n",
    "             f\"Output Dim: {output_dim}\\n\" \\\n",
    "             f\"Dropout Rate: {dropout_rate}\\n\" \\\n",
    "             f\"learning Rate: {learning_rate}\\n\" \\\n",
    "             f\"epochs: {num_epochs}\"\n",
    "print(params_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c1bac-04be-4871-aa67-9c3fdf87617e",
   "metadata": {},
   "source": [
    "### Train, Evaluate, and Predict Abstraction\n",
    "with 3,4 lines of code, you can almost train, evaluate any intent classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6d2d0cb-8ac2-49fb-92fa-73ae346bff47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.7383, Accuracy: 0.8312\n",
      "Epoch [2/5], Loss: 0.2805, Accuracy: 0.9312\n",
      "Epoch [3/5], Loss: 0.1614, Accuracy: 0.9592\n",
      "Epoch [4/5], Loss: 0.1452, Accuracy: 0.9640\n",
      "Epoch [5/5], Loss: 0.0954, Accuracy: 0.9756\n",
      "Test Loss: 0.3663\n",
      "Test Accuracy: 0.9400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick the model and train it. Evaluate the model on the test set.\n",
    "# choose model to train, uncomment the model you want to train and comment the other one\n",
    "# IntentClassifierLSTM is a simple LSTM model. IntentClassifierLSTMWithAttention is a LSTM model with attention.\n",
    "# The latter performs better.\n",
    "# Difference in Accuracy between the two models is about 3%\n",
    "\n",
    "# model = IntentClassifierLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "model = IntentClassifierLSTMWithAttention(vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(model, optimizer, loss_function, train_loader, num_epochs)\n",
    "evaluate(model, loss_function, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade7b12-6d50-4752-8424-d4a9d6b4733d",
   "metadata": {},
   "source": [
    "### Model and Tokenization Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9606f57c-9b43-4da0-9383-1e32030c0e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer for serving.\n",
    "model_name = \"IntentClassifierLSTMWithAttention\"\n",
    "torch.save(model.to(torch.device(\"cpu\")),f\"models/{model_name}.pth\")\n",
    "tokenizer.save_state(f\"models/{model_name}_tokenizer.pickle\", f\"models/{model_name}_le.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5138d87b-98d5-42ea-a6cb-f8c9fdb6b3ce",
   "metadata": {},
   "source": [
    "### Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbeff30b-8286-4ff0-b07c-e837fdda9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serve the model\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model_serve = torch.load(f\"models/{model_name}.pth\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8d8da-787a-4239-bc26-90ed2cbd797f",
   "metadata": {},
   "source": [
    "### Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5756d7fa-efef-4d2f-b386-2f572ae9dbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: ['airline']\n"
     ]
    }
   ],
   "source": [
    "# Predict on a query\n",
    "max_query_length = 50\n",
    "query_text = \"what airlines off from love field between 6 and 10 am on june sixth\"\n",
    "query = pd.DataFrame({\"text\": [query_text]})\n",
    "prediction = predict(model_serve, query,tokenizer,device)\n",
    "print(f\"Predicted label: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e4134-62e3-436e-88c4-811a0f4eca6c",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In the Notebooks, I have accomplished the following:\n",
    "1. Show how to train a simple model and add Attention Mechanism to improve accuracy\n",
    "2. Building a better model through hyper parameterization and Parameter Logging\n",
    "3. Model Management, Registry, and Experiment Management. Very important parts of Machine Learning Engineering\n",
    "4. Model Evaluation, on test data and performance during production time. Confidence Scores and Performance Improvment by using a distillation approach (using gpt4 to create OOS data and fine tune our best model). Improve production accuracy and performance\n",
    "5. Using Hugging Face Pretrained Transformers model. Fine tuning on Atis Data Set\n",
    "6. Building a Transfomer like Abstract Interface to ELSTM with Attention Model, i.e. Hide Pytorch and Only allow parameters to pass through (code in machine_learning folder)\n",
    "\n",
    "I hope many of the questions in the challenge are resolved. There are tons of things one can do there, one can visualize the impact of Attentions and Embeddings, one can implement A/B testing, logging in production, discuss more distillation approaches, etc.. looking forward to more fun :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682903e-e0bc-40af-8011-7b6e580c0548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniforge3-py3910]",
   "language": "python",
   "name": "conda-env-miniforge3-py3910-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
