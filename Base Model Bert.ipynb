{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5be6c7-927e-4a5d-a6f0-94d7f43a81f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T00:02:29.291934Z",
     "start_time": "2023-11-20T00:02:27.932706Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mcfg\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BertTokenizer, BertForSequenceClassification\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Trainer, TrainingArguments\n",
      "File \u001B[0;32m~/Documents/Job Search/Ultimate AI Challenge/ultimate_ai/config.py:25\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mmax\u001B[39m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Tokenize the text\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# This function splits text into words. Feel free to customize it as needed.\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Apply this function to the training and testing datasets\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m train_indices \u001B[38;5;241m=\u001B[39m [text_to_indices(text, word_to_index) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtrain\u001B[49m[\u001B[38;5;241m0\u001B[39m]]\n\u001B[1;32m     26\u001B[0m test_indices \u001B[38;5;241m=\u001B[39m [text_to_indices(text, word_to_index) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m test[\u001B[38;5;241m0\u001B[39m]]\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_indices[\u001B[38;5;241m42\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "import config as cfg\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device + \" is available\")\n",
    "\n",
    "# Load the pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the pre-trained model for sequence classification with the number of labels\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(cfg.le.classes_))\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6705cecf-4482-45fa-90b0-3d40b639a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Load data\n",
    "train = cfg.train\n",
    "test = cfg.test\n",
    "\n",
    "# Assume the second column is the label and the first column is the text\n",
    "train_texts = train[0].tolist()\n",
    "test_texts = test[0].tolist()\n",
    "\n",
    "# Convert labels to integer (if they are not already)\n",
    "# This might involve using a LabelEncoder as you have categorical labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = cfg.le\n",
    "num_labels = len(cfg.le.classes_)\n",
    "\n",
    "# Tokenize the text and create datasets\n",
    "max_length = 256  # You might need to adjust this based on your dataset\n",
    "train_dataset = IntentDataset(train_texts, cfg.train_labels, tokenizer, max_length)\n",
    "test_dataset = IntentDataset(test_texts, cfg.test_labels, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "329965f9-8bad-46aa-b2f3-b1a3ac1630c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4143003355.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[16], line 28\u001B[0;36m\u001B[0m\n\u001B[0;31m    trainer = Trainer(\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',  \n",
    "    logging_strategy=\"steps\",  # or \"epoch\"\n",
    "    logging_steps=50  # Log every 10 steps# directory for storing logs,\n",
    ")\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class MLflowLoggingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        # Log metrics with MLflow here\n",
    "        if metrics:\n",
    "            for key, value in metrics.items():\n",
    "                mlflow.log_metric(key, value, step=state.global_step)\n",
    "\n",
    "try:\n",
    "    mlflow.create_experiment(\"BertIntentClassification Pretrained\")\n",
    "    mlflow(pytorch=True)\n",
    "    mlflow.start_run()\n",
    "    mlflow.log_param(\"epochs\", training_args.num_train_epochs)\n",
    "    mlflow.log_param(\"batch_size\", training_args.per_device_train_batch_size)\n",
    "    mlflow.log_param(\"learning_rate\", training_args.learning_rate)\n",
    "    mlflow.log_param(\"weight_decay\", training_args.weight_decay)\n",
    "    mlflow.log_param(\"warmup_steps\", training_args.warmup_steps)\n",
    "    mlflow.log_param(\"max_length\", max_length)\n",
    "    mlflow.log_param(\"num_labels\", num_labels)\n",
    "    mlflow.log_param(\"model\", \"bert-base-uncased\")\n",
    "\n",
    "except:\n",
    "    pass\n",
    "#mlflow.log_params(your_params_dict)  # Log any initial parameters\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[MLflowLoggingCallback()]\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0063c44d-2a37-4d64-be03-8c463c47a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('results/final_bert_evaluated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1afde02-e977-4abd-8203-dc873dbe788f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1844148188829422,\n",
       " 'eval_runtime': 1.1111,\n",
       " 'eval_samples_per_second': 765.03,\n",
       " 'eval_steps_per_second': 12.601,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7876c729-309c-4c86-b28b-9cb75a8459e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_metric\n\u001B[1;32m      2\u001B[0m metric \u001B[38;5;241m=\u001B[39m load_metric(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2a799-b654-4147-9750-fafc8146a0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultimate_nlp",
   "language": "python",
   "name": "ultimate-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
