{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51fd1e24",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Bert Intent Classification\n",
    "An example Intent Classification model using BERT and HuggingFace Transformers. This notebook is based on the [HuggingFace Transformers documentation](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification). We can train a model to classify the intent of a user's message. For example, if a user says \"I want to buy a ticket\", the model should classify the intent as \"buy_ticket\". This is a common task in chatbots and virtual assistants.\n",
    "Steps:\n",
    "1. Load data\n",
    "2. Tokenize data\n",
    "3. Create PyTorch Dataset\n",
    "4. Train model\n",
    "5. Evaluate model\n",
    "6. Save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba54bca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:25:23.003867Z",
     "start_time": "2023-11-21T23:25:18.629779Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbreviation' 'aircraft' 'aircraft+flight+flight_no' 'airfare'\n",
      " 'airfare+flight_time' 'airline' 'airline+flight_no' 'airport' 'capacity'\n",
      " 'cheapest' 'city' 'distance' 'flight' 'flight+airfare' 'flight_no'\n",
      " 'flight_time' 'ground_fare' 'ground_service' 'ground_service+ground_fare'\n",
      " 'meal' 'quantity' 'restriction' '<unknown>']\n",
      "23\n",
      "Vocabulary size: 890\n",
      "Index of the word 'flight': 32\n",
      "[99, 100, 140, 15, 141, 142, 12, 97]\n",
      "[1, 93, 86, 3, 4, 5, 28, 64, 3, 68, 23, 260, 265]\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import config as cfg\n",
    "import warnings\n",
    "from transformers import TrainerCallback\n",
    "from machine_learning.model_utils import get_or_create_experiment\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bd028",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " ## Data Loading\n",
    "We will use the [ATIS dataset](https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk) from Kaggle. This dataset contains 5,047 sentences belonging to 26 intents. The sentences are queries from the ATIS (Airline Travel Information System) domain, and the intents are things like \"atis_flight\", \"atis_airfare\", \"atis_ground_service\", etc. The dataset is split into a training set and a test set. We will use the training set to train the model and the test set to evaluate the model. We will use the pretrained Bert tokenizer and model from HuggingFace. The tokenizer will convert the text into tokens that the model can understand. The model will be trained to classify the intent of the text. We will use the BertForSequenceClassification model, which is a pretrained Bert model with a single linear classification layer on top. This model can be used for sequence classification tasks like ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c74f208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:25:24.884345Z",
     "start_time": "2023-11-21T23:25:23.005060Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the pre-trained model for sequence classification with the number of labels\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(cfg.le.classes_))\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6eb0f1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## PyTorch Dataset\n",
    "The data set uses encodings from tokenizer and labels from label encoder. The data set is then used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6705cecf-4482-45fa-90b0-3d40b639a6ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:25:25.762342Z",
     "start_time": "2023-11-21T23:25:24.945933Z"
    }
   },
   "outputs": [],
   "source": [
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Load data\n",
    "train = cfg.train\n",
    "test = cfg.test\n",
    "\n",
    "# Assume the second column is the label and the first column is the text\n",
    "train_texts = train[0].tolist()\n",
    "test_texts = test[0].tolist()\n",
    "\n",
    "# Convert labels to integer (if they are not already)\n",
    "# This might involve using a LabelEncoder as you have categorical labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = cfg.le\n",
    "num_labels = len(cfg.le.classes_)\n",
    "\n",
    "# Tokenize the text and create datasets\n",
    "max_length = 256  # Max length of the text sequence, you might need to adjust this based on your dataset\n",
    "train_dataset = IntentDataset(train_texts, cfg.train_labels, tokenizer, max_length)\n",
    "test_dataset = IntentDataset(test_texts, cfg.test_labels, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7174b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Training\n",
    "Hyperparameters are defined here. The model is trained and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329965f9-8bad-46aa-b2f3-b1a3ac1630c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:29:58.967429Z",
     "start_time": "2023-11-21T23:25:25.766465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='870' max='870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [870/870 02:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.230800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.889900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.382900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.300500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.086300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.067900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.070300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.016600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',  \n",
    "    logging_strategy=\"steps\",  # or \"epoch\"\n",
    "    logging_steps=50  # Log every 10 steps# directory for storing logs,\n",
    ")\n",
    "\n",
    "class MLflowLoggingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        # Log metrics with MLflow here\n",
    "        if metrics:\n",
    "            for key, value in metrics.items():\n",
    "                mlflow.log_metric(key, value, step=state.global_step)\n",
    "\n",
    "try:\n",
    "    # Create an experiment and log parameters\n",
    "    get_or_create_experiment(\"BertIntentClassification Pretrained\")\n",
    "    mlflow(pytorch=True)\n",
    "    mlflow.start_run()\n",
    "    mlflow.log_param(\"epochs\", training_args.num_train_epochs)\n",
    "    mlflow.log_param(\"batch_size\", training_args.per_device_train_batch_size)\n",
    "    mlflow.log_param(\"learning_rate\", training_args.learning_rate)\n",
    "    mlflow.log_param(\"weight_decay\", training_args.weight_decay)\n",
    "    mlflow.log_param(\"warmup_steps\", training_args.warmup_steps)\n",
    "    mlflow.log_param(\"max_length\", max_length)\n",
    "    mlflow.log_param(\"num_labels\", num_labels)\n",
    "    mlflow.log_param(\"model\", \"bert-base-uncased\")\n",
    "\n",
    "except:\n",
    "    pass\n",
    "#mlflow.log_params(your_params_dict)  # Log any initial parameters\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[MLflowLoggingCallback()]\n",
    ")\n",
    "trainer.train()\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2facb0df",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed9cecb1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model('results/final_bert_evaluated')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea53d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " ## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1afde02-e977-4abd-8203-dc873dbe788f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.17989060282707214,\n",
       " 'eval_runtime': 0.9703,\n",
       " 'eval_samples_per_second': 875.999,\n",
       " 'eval_steps_per_second': 14.428,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958ef05-aff3-45ff-a1a6-75886424d57f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultimate_nlp",
   "language": "python",
   "name": "ultimate-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
